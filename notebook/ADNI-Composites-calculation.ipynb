{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook used for the calculation of the composite scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "PATH_TO_ROOT = Path(os.path.join('..'))\n",
    "\n",
    "# add paths to internal libraries\n",
    "sys.path.append(str(PATH_TO_ROOT / 'src'))\n",
    "\n",
    "from utils.variables import GOJO_VERSION\n",
    "\n",
    "# import gojo modules\n",
    "gojo = importlib.import_module(GOJO_VERSION)\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data processed using \"Pipeline - Clinical data.ipynb\"\n",
    "PATH_TO_STUDY_DATA = PATH_TO_ROOT / 'data' / 'adni' / 'processed'\n",
    "PATH_TO_NEUROPSYCHO_DATA = PATH_TO_STUDY_DATA / '20240428_neuropsycho.parquet'\n",
    "PATH_TO_NEUROLOGY_DATA = PATH_TO_STUDY_DATA / '20240428_diagnosis.parquet'\n",
    "PATH_TO_MMSE_CDR_DATA = PATH_TO_STUDY_DATA / '20240428_mmse_cdr.parquet'\n",
    "PATH_TO_AMY_STATUS_DATA = PATH_TO_STUDY_DATA / '..' / 'download' / 'UCBERKELEY_AMY_6MM_24Apr2024.csv'\n",
    "\n",
    "# composite structure\n",
    "SEM_STRUCTURE = {\n",
    "    \n",
    "    'memory': [\n",
    "        'memory_avlt_trial_1',\n",
    "        'memory_avlt_trial_2',\n",
    "        #'memory_avlt_trial_3',\n",
    "        #'memory_avlt_trial_4',\n",
    "        #'memory_avlt_trial_5',\n",
    "        'memory_avlt_trial_6',\n",
    "        'memory_avlt_delayed',\n",
    "        'memory_avlt_recognition',\n",
    "        'memory_word_recognition',\n",
    "        'memory_word_recall_delayed',\n",
    "        'memory_word_recall',\n",
    "        #'memory_remembering_test',\n",
    "    ],\n",
    "    'language': [\n",
    "        'language_cat_fluency',\n",
    "        'language_naming', \n",
    "        'language_bnt_tot',\n",
    "        'language_word_finding_diff',\n",
    "    ],\n",
    "    'exec': [\n",
    "        'exec_tmt_a_time',\n",
    "        'exec_tmt_b_time',\n",
    "        # 'attention_digit_span_forward_binary',\n",
    "        # 'attention_digit_span_backward_binary',\n",
    "        'attention_number_cancellation'\n",
    "    ],\n",
    "    'visuospatial': [\n",
    "\n",
    "        'visuos_clock_copy_tot_score',\n",
    "        'visuos_clock_draw_tot_score',\n",
    "        'visuos_constructional_praxis',\n",
    "        'visuos_ideational_praxis',\n",
    "    ]\n",
    "}\n",
    "\n",
    "# parameters used to perform the imputation\n",
    "IMPUATION_MODEL = {\n",
    "    'model': RandomForestRegressor,\n",
    "    'model_params': dict(\n",
    "        n_estimators=400, \n",
    "        max_depth=5,\n",
    "        min_samples_split=50,\n",
    "        min_samples_leaf=20,\n",
    "        max_features=0.75,\n",
    "        max_samples=0.75,\n",
    "        bootstrap=True,\n",
    "        n_jobs=20,\n",
    "        random_state=1997\n",
    "    ),\n",
    "    'imputation_params': dict(\n",
    "        max_iter=50,\n",
    "        initial_strategy='median'\n",
    "    ) \n",
    "}\n",
    "\n",
    "\n",
    "# variables that will not be rounded to the nearest integer after imputation\n",
    "CONTINUOUS_VARIABLES = [\n",
    "    'exec_tmt_a_time',\n",
    "    'exec_tmt_b_time',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossNearestInfo(\n",
    "    target_df: pd.DataFrame,\n",
    "    source_df: pd.DataFrame,\n",
    "    how: str,\n",
    "    window: int = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Function used to cross dataframes by nearest date \"\"\"\n",
    "    # get date columns\n",
    "    subject_id = target_df.index.names[0]\n",
    "    target_date = target_df.index.names[1]\n",
    "    source_date = source_df.index.names[1]\n",
    "    \n",
    "    # cross information\n",
    "    crossed_df = target_df.join(source_df, how=how).reset_index([target_date, source_date])\n",
    "    \n",
    "    # select information in window\n",
    "    if window:\n",
    "        crossed_df = crossed_df.loc[(crossed_df[target_date] - crossed_df[source_date]).dt.days.abs() < window].copy()\n",
    "    \n",
    "    # remove possible duplicates\n",
    "    crossed_df['_days_diff'] = (crossed_df[target_date] - crossed_df[source_date]).dt.days.abs()\n",
    "    crossed_df = crossed_df.reset_index().set_index([subject_id, target_date, '_days_diff']).sort_index()\n",
    "    crossed_df = crossed_df.groupby([subject_id, target_date]).nth(0)\n",
    "    crossed_df = crossed_df.reset_index('_days_diff').drop(columns=['_days_diff']).copy()\n",
    "    \n",
    "    assert not crossed_df.index.duplicated().any()\n",
    "\n",
    "    return crossed_df\n",
    "\n",
    "\n",
    "def getValsToImpute(arr: np.ndarray):\n",
    "    \"\"\" Subroutine used to calculate the vals to be imputed \"\"\"\n",
    "\n",
    "    length = len(arr)\n",
    "    \n",
    "    # forward pass\n",
    "    vals_to_impute = []\n",
    "    start_to_imputate = False\n",
    "    for i in range(length):\n",
    "        if np.isnan(arr[i]) and not start_to_imputate:\n",
    "            vals_to_impute.append(False)\n",
    "            continue\n",
    "\n",
    "        vals_to_impute.append(True)\n",
    "        start_to_imputate = True\n",
    "\n",
    "    # backward pass\n",
    "    start_to_imputate = False\n",
    "    for i in range(length):\n",
    "        curr_idx = length - i - 1\n",
    "        if not vals_to_impute[curr_idx]:\n",
    "            continue\n",
    "            \n",
    "        if np.isnan(arr[curr_idx]) and not start_to_imputate:\n",
    "            vals_to_impute[curr_idx] = False\n",
    "            continue\n",
    "\n",
    "        if np.isnan(arr[curr_idx]):\n",
    "            vals_to_impute[curr_idx] = True\n",
    "            \n",
    "        start_to_imputate = True\n",
    "\n",
    "    # non-missing values won't be imputed\n",
    "    for i in range(length):\n",
    "        if not np.isnan(arr[i]):\n",
    "            vals_to_impute[i] = False\n",
    "            \n",
    "\n",
    "    return np.array(vals_to_impute)\n",
    "\n",
    "\n",
    "def interpolateMidPoints(\n",
    "        df: pd.DataFrame, \n",
    "        var: str, \n",
    "        unique_ids: np.ndarray, \n",
    "        display_plots: int = 0\n",
    "):\n",
    "    \"\"\" Subroutine used to interpolate missing values \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # make an interpolation of the values that are in the middle of the trayectory\n",
    "    for idx, subid in enumerate(unique_ids):\n",
    "        sub_data = df.loc[subid, [var, 'years_followup_neurobat']].values\n",
    "        bnt_vals = sub_data[:, 0]\n",
    "        time_step = sub_data[:, 1]\n",
    "        na_mask = np.isnan(bnt_vals)\n",
    "    \n",
    "        # when all values are known continue (better prevent)\n",
    "        if np.all(~na_mask):\n",
    "            continue\n",
    "            \n",
    "        # when there is less than 4 values continue\n",
    "        if (~na_mask).sum() < 3:\n",
    "            continue\n",
    "    \n",
    "        # check for valid values to imputate\n",
    "        vals_to_imputate = getValsToImpute(bnt_vals)\n",
    "        if np.all(vals_to_imputate == False):\n",
    "            continue\n",
    "    \n",
    "        # adjust the spline (monotonic interpolation)\n",
    "        spl = PchipInterpolator(time_step[~na_mask], bnt_vals[~na_mask])\n",
    "        bnt_vals[vals_to_imputate] = spl(time_step[vals_to_imputate])\n",
    "        df.loc[subid, var] = bnt_vals\n",
    "\n",
    "        if idx < display_plots:\n",
    "            fig, ax = plt.subplots(figsize=(5, 2.5))\n",
    "            plt.scatter(\n",
    "                time_step,\n",
    "                bnt_vals)\n",
    "            \n",
    "            plt.scatter(\n",
    "                time_step[vals_to_imputate],\n",
    "                spl(time_step[vals_to_imputate]),\n",
    "                color='red', s=75\n",
    "            )\n",
    "            plt.plot(\n",
    "                time_step,\n",
    "                bnt_vals)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.grid(alpha=0.3, color='black')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title(var)\n",
    "            plt.show()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(PATH_TO_NEUROPSYCHO_DATA), 'PATH_TO_NEUROPSYCHO_DATA not found'\n",
    "assert os.path.exists(PATH_TO_NEUROLOGY_DATA), 'PATH_TO_NEUROLOGY_DATA not found'\n",
    "assert os.path.exists(PATH_TO_MMSE_CDR_DATA), 'PATH_TO_MMSE_CDR_DATA not found'\n",
    "\n",
    "# read the data\n",
    "neuro = pd.read_parquet(PATH_TO_NEUROPSYCHO_DATA)\n",
    "diag = pd.read_parquet(PATH_TO_NEUROLOGY_DATA)\n",
    "mmse_cdr = pd.read_parquet(PATH_TO_MMSE_CDR_DATA)\n",
    "\n",
    "# merge neuropsycho and diagnostic information\n",
    "neuro = crossNearestInfo(target_df=neuro, source_df=diag, how='left', window=30*3)\n",
    "\n",
    "# add the MMSE/CDR information\n",
    "neuro = crossNearestInfo(target_df=neuro, source_df=mmse_cdr, how='left', window=30*3)\n",
    "\n",
    "# calculate the follow-up time\n",
    "neuro['_date'] = neuro.index.get_level_values('neurobat_date')\n",
    "neuro['years_followup_neurobat'] = neuro.groupby('subject_id')['_date'].diff().dt.days.fillna(0.0).groupby('subject_id').cumsum() / 365\n",
    "neuro = neuro.drop(columns=['_date'])\n",
    "\n",
    "# select the variables used for the composites creation\n",
    "composite_source_vars = [test for tests in SEM_STRUCTURE.values() for test in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Process memory window\n",
    "neuro.loc[neuro.memory_avlt_recognition == -1, 'memory_avlt_recognition'] = np.nan\n",
    "neuro.loc[neuro.memory_avlt_trial_1 == -1, 'memory_avlt_trial_1'] = np.nan\n",
    "neuro.loc[neuro.memory_avlt_trial_2 == -1, 'memory_avlt_trial_2'] = np.nan\n",
    "neuro.loc[neuro.memory_avlt_trial_6 == -1, 'memory_avlt_trial_6'] = np.nan\n",
    "neuro.loc[neuro.memory_avlt_delayed == -1, 'memory_avlt_delayed'] = np.nan\n",
    "neuro.loc[neuro.memory_word_recall < 0, 'memory_word_recall'] = np.nan\n",
    "neuro.loc[neuro.memory_word_recall_delayed < 0, 'memory_word_recall_delayed'] = np.nan\n",
    "neuro.loc[neuro.memory_word_recognition < 0, 'memory_word_recognition'] = np.nan\n",
    "\n",
    "neuro['memory_word_recognition'] = -1*neuro['memory_word_recognition']\n",
    "neuro['memory_word_recall_delayed'] = -1*neuro['memory_word_recall_delayed']\n",
    "neuro['memory_word_recall'] = -1*neuro['memory_word_recall']\n",
    "\n",
    "\n",
    "# --- Process language window\n",
    "neuro.loc[neuro.language_naming < 0, 'language_naming'] = np.nan\n",
    "neuro.loc[neuro.language_cat_fluency < 0, 'language_cat_fluency'] = np.nan\n",
    "neuro.loc[neuro.language_word_finding_diff < 0, 'language_word_finding_diff'] = np.nan\n",
    "neuro.loc[neuro.language_bnt_tot < 0, 'language_bnt_tot'] = np.nan\n",
    "\n",
    "# Convert variable to categorical with ordinal categories of (2 = no difficulties, 1 = some difficulties, 0 = difficulties)\n",
    "neuro.loc[neuro['language_naming'] == 0.0, 'language_naming'] = 0\n",
    "neuro.loc[neuro['language_naming'] == 1.0, 'language_naming'] = 1\n",
    "neuro.loc[neuro['language_naming'] > 1.0, 'language_naming'] = 2\n",
    "neuro['language_naming'] = (-1 * neuro['language_naming']) + 2\n",
    "neuro['language_word_finding_diff'] = -1*neuro['language_word_finding_diff']\n",
    "\n",
    "\n",
    "# --- Process executive window\n",
    "neuro.loc[neuro['exec_tmt_a_time']  < 10, 'exec_tmt_a_time'] = np.nan\n",
    "neuro.loc[neuro['exec_tmt_b_time']  < 10, 'exec_tmt_b_time'] = np.nan\n",
    "neuro.loc[neuro.attention_number_cancellation < 0, 'attention_number_cancellation'] = np.nan\n",
    "\n",
    "# convert TMT times to log scale\n",
    "neuro['exec_tmt_a_time'] = np.log(neuro['exec_tmt_a_time'])\n",
    "neuro['exec_tmt_b_time'] = np.log(neuro['exec_tmt_b_time'])\n",
    "\n",
    "# invert TMT tests\n",
    "neuro['exec_tmt_a_time'] = -1*neuro['exec_tmt_a_time']\n",
    "neuro['exec_tmt_b_time'] = -1*neuro['exec_tmt_b_time']\n",
    "\n",
    "\n",
    "# invert ADAS-COG number cancellation\n",
    "neuro['attention_number_cancellation'] = (-1*neuro['attention_number_cancellation'] + 5)\n",
    "\n",
    "\n",
    "# --- Process visuos window\n",
    "neuro.loc[neuro.visuos_constructional_praxis < 0, 'visuos_constructional_praxis'] = np.nan\n",
    "neuro.loc[neuro.visuos_clock_copy_tot_score < 0, 'visuos_clock_copy_tot_score'] = np.nan\n",
    "neuro.loc[neuro.visuos_clock_draw_tot_score < 0, 'visuos_clock_draw_tot_score'] = np.nan\n",
    "neuro.loc[neuro.visuos_ideational_praxis < 0, 'visuos_ideational_praxis'] = np.nan\n",
    "# invert praxis scores\n",
    "neuro['visuos_constructional_praxis'] = (-1 * neuro['visuos_constructional_praxis']) + 5\n",
    "neuro['visuos_ideational_praxis'] = (-1 * neuro['visuos_ideational_praxis']) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for domain, tests in SEM_STRUCTURE.items():\n",
    "        \n",
    "        # display composites distribution\n",
    "        n_subplots = len(tests)\n",
    "        num_rows = (n_subplots + 2) // 3\n",
    "        fig, axes = plt.subplots(num_rows, 3, figsize=(14, 3.5 * num_rows))\n",
    "        axes = axes.flatten() if num_rows > 0 else [axes]\n",
    "        for i, (var, ax) in enumerate(zip(tests, axes)):\n",
    "            sns.boxplot(\n",
    "                data=neuro,\n",
    "                y=var,\n",
    "                hue='diagnosis',\n",
    "                hue_order=['control', 'mci', 'dementia'],\n",
    "                ax=ax,\n",
    "                gap=0.25,\n",
    "                palette='Blues'\n",
    "            )\n",
    "            sns.move_legend(\n",
    "                ax, \"lower center\",\n",
    "                bbox_to_anchor=(.5, -0.25), ncol=3, title=None, frameon=False,\n",
    "            )\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.grid(alpha=.3, color='black')\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('score')\n",
    "            ax.set_title(var, size=14, pad=15)\n",
    "            \n",
    "        for j in range(i + 1, num_rows * 3):\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.subplots_adjust(hspace=0.75, wspace=0.5)\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuro[composite_source_vars].describe().loc[[\n",
    "    'min', 'max', 'mean', 'std'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    data=neuro[composite_source_vars].corr(),\n",
    "    annot=True, fmt=\".2f\",\n",
    "    vmin=0, vmax=1,\n",
    "    cmap='Spectral_r'\n",
    ")\n",
    "\n",
    "# group test by cognitive domain\n",
    "groups = {}\n",
    "i = 0\n",
    "for domain, tests in SEM_STRUCTURE.items():\n",
    "    groups[domain] = (i, i+len(tests)-1)\n",
    "    i += len(tests)\n",
    "    \n",
    "for group, (start, end) in groups.items():\n",
    "    # Añadir un rectángulo alrededor del grupo\n",
    "    rect = plt.Rectangle((start, start), end-start+1, end-start+1, fill=False, edgecolor='black', lw=2)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original data for calculating the composite scores\n",
    "neuro_no_impt = neuro.copy()\n",
    "\n",
    "# missing values\n",
    "(pd.DataFrame(\n",
    "    neuro[composite_source_vars].isna().sum(), columns=['perc_missing']\n",
    ").sort_values(by='perc_missing', ascending=False) / neuro.shape[0] * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a monotonic B-spline interpolation of the intermediate missing values\n",
    "for idx, var in enumerate(composite_source_vars):\n",
    "    subject_ids = neuro.loc[neuro[var].isna()].index.get_level_values('subject_id').unique().values\n",
    "    neuro = interpolateMidPoints(\n",
    "        df=neuro,\n",
    "        var=var,\n",
    "        unique_ids=subject_ids,\n",
    "        display_plots=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "(pd.DataFrame(\n",
    "    neuro[composite_source_vars].isna().sum(), columns=['perc_missing']\n",
    ").sort_values(by='perc_missing', ascending=False) / neuro.shape[0] * 100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the entries with missing values in the BNT\n",
    "neuro_bnt_nans_index = neuro.loc[\n",
    "    neuro.language_bnt_tot.isna()\n",
    "].index\n",
    "\n",
    "# add diagnosis as categorical variable\n",
    "neuro = neuro.join(pd.get_dummies(neuro['diagnosis'], drop_first=True).astype(int))\n",
    "\n",
    "np.random.seed(1997)\n",
    "\n",
    "# create imputation instance\n",
    "imputer = IterativeImputer(\n",
    "    estimator=IMPUATION_MODEL['model'](**IMPUATION_MODEL['model_params']),\n",
    "    min_value=neuro[composite_source_vars + ['age', 'yschooling', 'mci', 'dementia']].min(),\n",
    "    max_value=neuro[composite_source_vars + ['age', 'yschooling', 'mci', 'dementia']].max(),\n",
    "    random_state=1997,\n",
    "    verbose=1,\n",
    "    **IMPUATION_MODEL['imputation_params']\n",
    ")\n",
    "\n",
    "# perform imputation\n",
    "neuro_imputed = imputer.fit_transform(neuro[composite_source_vars + ['age', 'yschooling', 'mci', 'dementia']])\n",
    "neuro_imputed = pd.DataFrame(neuro_imputed, columns=composite_source_vars + ['age', 'yschooling', 'mci', 'dementia'])\n",
    "neuro[composite_source_vars] = neuro_imputed[composite_source_vars].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert non-float variables to integer\n",
    "for var in composite_source_vars:\n",
    "    if var not in CONTINUOUS_VARIABLES:\n",
    "        neuro[var] = neuro[var].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case: BNT imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select BNT subjects as NaNs\n",
    "neuro.loc[neuro_bnt_nans_index, 'language_bnt_tot'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model used to imputate the BNT\n",
    "model = gojo.interfaces.SklearnModelWrapper(\n",
    "    model_class=IMPUATION_MODEL['model'],\n",
    "    **IMPUATION_MODEL['model_params']\n",
    ")\n",
    "model.updateParameters(n_jobs=1)    # select only one worker\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the X and y data\n",
    "X = neuro.loc[~neuro['language_bnt_tot'].isna()][\n",
    "    [v for v in composite_source_vars if v != 'language_bnt_tot'] + [\n",
    "     'age', 'yschooling', 'mci', 'dementia']\n",
    "]\n",
    "y = neuro.loc[~neuro['language_bnt_tot'].isna()]['language_bnt_tot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model via cross validation to obtain an estimate of the expected error\n",
    "# during imputation\n",
    "cv_report = gojo.core.evalCrossVal(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    model=model,\n",
    "    cv=gojo.util.splitter.InstanceLevelKFoldSplitter(\n",
    "        n_splits=10, n_repeats=5, instance_id=X.index.get_level_values('subject_id').values),\n",
    "    n_jobs=10,\n",
    "    save_train_preds=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions vs true values\n",
    "sns.lmplot(\n",
    "    data=cv_report.getTestPredictions(),\n",
    "    y='pred_labels',\n",
    "    x='true_labels',\n",
    "    aspect=1.25,\n",
    "    scatter_kws=dict(color='grey', s=20, alpha=0.2),\n",
    "    line_kws=dict(color='red', lw=2),\n",
    "    height=3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display performance on the test set\n",
    "cv_report.getScores(\n",
    "    gojo.core.getDefaultMetrics('regression'), supress_warnings=True)['test'].drop(columns=['n_fold']).round(decimals=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display performance on the training set\n",
    "cv_report.getScores(\n",
    "    gojo.core.getDefaultMetrics('regression'), supress_warnings=True)['train'].drop(columns=['n_fold']).round(decimals=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to predict subjects without any existing value in the BNT\n",
    "model.train(X, y)\n",
    "pred_bnt_scores = model.performInference(\n",
    "    neuro.loc[neuro['language_bnt_tot'].isna()][\n",
    "        [v for v in composite_source_vars if v != 'language_bnt_tot'] + [\n",
    "         'age', 'yschooling', 'mci', 'dementia']\n",
    "    ]\n",
    ")\n",
    "\n",
    "# save the predictions in another different variable\n",
    "neuro['language_bnt_tot_rf_pred'] = np.nan\n",
    "neuro.loc[neuro['language_bnt_tot'].isna(), 'language_bnt_tot_rf_pred'] = pred_bnt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a LMM model to the longitudinal data\n",
    "lmm_input = neuro.loc[~neuro['language_bnt_tot'].isna()][[\n",
    "    'language_bnt_tot',\n",
    "    'age',\n",
    "    'yschooling',\n",
    "    'dementia',\n",
    "    'mci',\n",
    "    'years_followup_neurobat',\n",
    "]].copy()\n",
    "lmm_input['age'] = lmm_input['age'] -  lmm_input['years_followup_neurobat']   # remove time effect from the age\n",
    "\n",
    "# adjustr the LMM model\n",
    "lmm_model = smf.mixedlm(\n",
    "    \"\"\" language_bnt_tot ~ 1 + years_followup_neurobat +\n",
    "        years_followup_neurobat*dementia +\n",
    "        years_followup_neurobat*mci +\n",
    "        age + \n",
    "        yschooling\n",
    "    \"\"\", \n",
    "    re_formula=\"~ 1 + years_followup_neurobat\",\n",
    "    data=lmm_input.reset_index(), \n",
    "    groups=lmm_input.reset_index()[\"subject_id\"],\n",
    "    missing='drop'\n",
    ")\n",
    "lmm_result = lmm_model.fit(method=[\"lbfgs\"])\n",
    "\n",
    "# extract the model coefficients\n",
    "model_coefs = lmm_result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R^2: {:.3f}'.format(r2_score(lmm_input['language_bnt_tot'], lmm_result.fittedvalues)))\n",
    "lmm_result.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the imputation for each subject\n",
    "sub_ids = neuro.loc[neuro['language_bnt_tot'].isna()].index.get_level_values('subject_id').unique()\n",
    "\n",
    "neuro['language_bnt_tot_imputed'] = np.nan\n",
    "max_display = 10\n",
    "display_count = 0\n",
    "for subid in sub_ids:\n",
    "    true_vals = neuro.loc[subid, 'language_bnt_tot'].copy().values\n",
    "    pred_vals = neuro.loc[subid, 'language_bnt_tot_rf_pred'].copy().values\n",
    "\n",
    "    # imputate the first value using the predicted value of the model\n",
    "    if np.all(np.isnan(true_vals)):\n",
    "        true_vals[0] = pred_vals[0]\n",
    "\n",
    "    # impute the values using the expected decay based on the LMM parameters\n",
    "    if not np.all(~np.isnan(true_vals)):\n",
    "        first_nan_idx = np.where(np.isnan(true_vals))[0][0]\n",
    "        true_vals[first_nan_idx:] = true_vals[first_nan_idx-1] + (\n",
    "            model_coefs.loc['years_followup_neurobat'] * neuro.loc[subid, 'years_followup_neurobat'].values[first_nan_idx:] + \n",
    "            model_coefs.loc['dementia'] * neuro.loc[subid, 'dementia'].values[first_nan_idx:] +\n",
    "            model_coefs.loc['mci'] * neuro.loc[subid, 'mci'].values[first_nan_idx:] +\n",
    "            model_coefs.loc['years_followup_neurobat:dementia'] * neuro.loc[subid, 'dementia'].values[first_nan_idx:] * neuro.loc[subid, 'years_followup_neurobat'].values[first_nan_idx:] +\n",
    "            model_coefs.loc['years_followup_neurobat:mci'] * neuro.loc[subid, 'mci'].values[first_nan_idx:] * neuro.loc[subid, 'years_followup_neurobat'].values[first_nan_idx:]\n",
    "        )\n",
    "        \n",
    "    # save the imputed values        \n",
    "    neuro.loc[subid, 'language_bnt_tot_imputed'] = true_vals\n",
    "\n",
    "    if display_count < max_display and np.random.uniform() > 0.5:\n",
    "        display_count += 1\n",
    "        fig, ax = plt.subplots(figsize=(4, 2))\n",
    "        \n",
    "        ax.plot(\n",
    "            neuro.loc[subid, 'years_followup_neurobat'].values,\n",
    "            neuro.loc[subid, 'language_bnt_tot_imputed'], color='orange'\n",
    "        )\n",
    "        ax.scatter(\n",
    "            neuro.loc[subid, 'years_followup_neurobat'].values,\n",
    "            neuro.loc[subid, 'language_bnt_tot_imputed'], color='orange'\n",
    "        )\n",
    "        ax.plot(\n",
    "            neuro.loc[subid, 'years_followup_neurobat'].values,\n",
    "            neuro.loc[subid, 'language_bnt_tot'].values,\n",
    "            color='blue'\n",
    "        )\n",
    "        ax.scatter(\n",
    "            neuro.loc[subid, 'years_followup_neurobat'].values,\n",
    "            neuro.loc[subid, 'language_bnt_tot'].values,\n",
    "            color='blue'\n",
    "        )\n",
    "        ax.grid(alpha=0.3, color='black')\n",
    "        for pos in ['left', 'right', 'bottom', 'top']:\n",
    "            ax.spines[pos].set_visible(False)\n",
    "        ax.set_xlabel('Years')\n",
    "        ax.set_ylabel('BNT')\n",
    "        ax.set_ylim(-1, 31)\n",
    "        plt.show()\n",
    "\n",
    "# adjust imputed values range\n",
    "neuro.loc[neuro['language_bnt_tot_imputed'] > 30, 'language_bnt_tot_imputed'] = 30\n",
    "neuro.loc[neuro['language_bnt_tot_imputed'] < 0, 'language_bnt_tot_imputed'] = 0\n",
    "neuro.loc[~neuro['language_bnt_tot'].isna(), 'language_bnt_tot_imputed'] =\\\n",
    "    neuro.loc[~neuro['language_bnt_tot'].isna(), 'language_bnt_tot'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display model predictions vs imputed values \n",
    "sns.lmplot(\n",
    "    data=neuro,\n",
    "    y='language_bnt_tot_rf_pred',\n",
    "    x='language_bnt_tot_imputed',\n",
    "    aspect=1.25,\n",
    "    scatter_kws=dict(color='grey', s=20, alpha=0.2),\n",
    "    line_kws=dict(color='red', lw=2),\n",
    "    height=3\n",
    ")\n",
    "plt.title('Correlation {:.3f}'.format(\n",
    "    neuro.loc[neuro['language_bnt_tot'].isna()][[\n",
    "        'language_bnt_tot_imputed', 'language_bnt_tot_rf_pred']].corr().values[0, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the BNT original version with the imputed one\n",
    "neuro['language_bnt_tot_original'] = neuro['language_bnt_tot']\n",
    "neuro['language_bnt_tot'] = neuro['language_bnt_tot_imputed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "pd.DataFrame(\n",
    "    neuro[composite_source_vars].isna().sum(), columns=['perc_missing']\n",
    ").sort_values(by='perc_missing', ascending=False) / neuro.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for domain, tests in SEM_STRUCTURE.items():\n",
    "        \n",
    "        # display composites distribution\n",
    "        n_subplots = len(tests)\n",
    "        num_rows = (n_subplots + 2) // 3\n",
    "        fig, axes = plt.subplots(num_rows, 3, figsize=(14, 3.5 * num_rows))\n",
    "        axes = axes.flatten() if num_rows > 0 else [axes]\n",
    "        for i, (var, ax) in enumerate(zip(tests, axes)):\n",
    "            sns.boxplot(\n",
    "                data=neuro,\n",
    "                y=var,\n",
    "                hue='diagnosis',\n",
    "                hue_order=['control', 'mci', 'dementia'],\n",
    "                ax=ax,\n",
    "                gap=0.25,\n",
    "                palette='Blues'\n",
    "            )\n",
    "            sns.move_legend(\n",
    "                ax, \"lower center\",\n",
    "                bbox_to_anchor=(.5, -0.25), ncol=3, title=None, frameon=False,\n",
    "            )\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.grid(alpha=.3, color='black')\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('score')\n",
    "            ax.set_title(var, size=14, pad=15)\n",
    "            \n",
    "        for j in range(i + 1, num_rows * 3):\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.subplots_adjust(hspace=0.75, wspace=0.5)\n",
    "        plt.show()\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(\n",
    "    data=neuro[composite_source_vars].corr(),\n",
    "    annot=True, fmt=\".2f\",\n",
    "    vmin=0, vmax=1,\n",
    "    cmap='Spectral_r'\n",
    ")\n",
    "\n",
    "# group test by cognitive domain\n",
    "groups = {}\n",
    "i = 0\n",
    "for domain, tests in SEM_STRUCTURE.items():\n",
    "    groups[domain] = (i, i+len(tests)-1)\n",
    "    i += len(tests)\n",
    "    \n",
    "for group, (start, end) in groups.items():\n",
    "    # Añadir un rectángulo alrededor del grupo\n",
    "    rect = plt.Rectangle((start, start), end-start+1, end-start+1, fill=False, edgecolor='black', lw=2)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix variables with a low number of levels\n",
    "original_neuro = neuro_no_impt[composite_source_vars].dropna().sort_index().groupby('subject_id').nth(-1).copy()\n",
    "original_neuro.loc[original_neuro['visuos_constructional_praxis'] < 1.5, 'visuos_constructional_praxis'] = 2.0\n",
    "neuro.loc[neuro['visuos_constructional_praxis'] < 1.5, 'visuos_constructional_praxis'] = 2.0\n",
    "\n",
    "# export the generated data to calculate the composites using R. Use the data without imputation for model fitting\n",
    "original_neuro.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_STUDY_DATA, '%s_neuropsycho_inputSEM_last.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")\n",
    "\n",
    "neuro[composite_source_vars].sort_index().to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_STUDY_DATA, '%s_neuropsycho_inputSEM_allObs.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composites calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the calculated composites\n",
    "composites = pd.read_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_STUDY_DATA, '20240428_neuropsycho_lavaan_SEM.parquet'\n",
    "    )\n",
    ")\n",
    "\n",
    "# format the loaded dataframe\n",
    "composites['neurobat_date'] = pd.to_datetime(composites['neurobat_date'].apply(lambda v: str(v)[:11]))\n",
    "composites = composites.set_index(['subject_id', 'neurobat_date']).sort_index()\n",
    "composites.columns = ['%s_composite' % c for c in composites.columns]\n",
    "\n",
    "# add the composites to the neuropsychological data\n",
    "neuro = neuro.join(composites, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normative data adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- adjust the values by age and education level using normative information (controls that mantain their diagnosis)\n",
    "# select normative subjects (baseline diagnosis control, last diagnosis control, and at least 2 years of follow-up)\n",
    "# with negative amyloid status\n",
    "\n",
    "# select subjects with negative amyloid status based on:\n",
    "# https://doi.org/10.1016/j.jalz.2012.10.007\n",
    "uc_amy_status = pd.read_csv(PATH_TO_AMY_STATUS_DATA, low_memory=False)\n",
    "uc_amy_status = uc_amy_status[['PTID', 'SCANDATE', 'AMYLOID_STATUS_COMPOSITE_REF', 'SUMMARY_SUVR']].copy()\n",
    "uc_amy_status['SCANDATE'] = pd.to_datetime(uc_amy_status['SCANDATE'])\n",
    "uc_amy_status = uc_amy_status.rename(columns={'PTID': 'subject_id', 'SCANDATE': 'date'}).set_index(['subject_id', 'date'])\n",
    "uc_amy_status_last = uc_amy_status.sort_index().groupby('subject_id').nth(-1)\n",
    "amy_neg_subject_ids = uc_amy_status_last\\\n",
    "    .loc[uc_amy_status_last['AMYLOID_STATUS_COMPOSITE_REF'] < 0.5]\\\n",
    "    .index.get_level_values('subject_id').unique()\n",
    "print(f'Number of subjects with a negative amyloid status: {len(amy_neg_subject_ids)}')\n",
    "\n",
    "# select baseline and last information\n",
    "base_neuro = neuro.sort_index().groupby('subject_id').nth(0).reset_index('neurobat_date')\n",
    "last_neuro = neuro.sort_index().groupby('subject_id').nth(-1).reset_index('neurobat_date')\n",
    "base_neuro.columns = ['base_%s' % c for c in base_neuro.columns]\n",
    "last_neuro.columns = ['last_%s' % c for c in last_neuro.columns]\n",
    "base_last_neuro = base_neuro.join(last_neuro, how='inner')\n",
    "\n",
    "# remove subjects with less than 2 years of follow-up\n",
    "base_last_neuro = base_last_neuro.loc[\n",
    "    (base_last_neuro['last_diag_date'] - base_last_neuro['base_diag_date']).dt.days.abs() >= 365*2\n",
    "].copy()\n",
    "\n",
    "print('Number of subjects with al least 2 years of follow-up: %d' % len(base_last_neuro))\n",
    "\n",
    "# select subjects that mantain a healthy diagnosis with a CDR == 0 and\n",
    "# select last evaluation and mmse >= 26 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6949533/)\n",
    "normative_subjects = base_last_neuro.loc[\n",
    "    base_last_neuro['base_diagnosis'].isin(['control']) &\n",
    "    base_last_neuro['last_diagnosis'].isin(['control']) &\n",
    "    base_last_neuro['base_cdr'].isin([0.0]) &\n",
    "    base_last_neuro['last_cdr'].isin([0.0]) &\n",
    "    (base_last_neuro['last_mmse'] >= 26) & \n",
    "    base_last_neuro.index.isin(amy_neg_subject_ids)\n",
    "].copy()\n",
    "\n",
    "print('Number of control subjects used to adjust cognitive neuro: %d' % len(normative_subjects))\n",
    "\n",
    "# save information about amyloid status\n",
    "neuro['uc_berkley_negative_amyloid_1Y'] = np.nan\n",
    "neuro.loc[\n",
    "    neuro.index.get_level_values('subject_id').isin(normative_subjects.index.unique()), \n",
    "    'uc_berkley_negative_amyloid_1Y'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center independent variables\n",
    "Xvar = normative_subjects[['last_age', 'last_yschooling']].copy()\n",
    "Xvar_mean = Xvar.mean()\n",
    "Xvar = Xvar - Xvar_mean\n",
    "Xvar = sm.add_constant(Xvar)\n",
    "\n",
    "# adjust variables\n",
    "for composite in composites.columns:\n",
    "    print('\\n =============== Composite: \"%s\"\\n' % composite)\n",
    "    \n",
    "    base_composite = 'base_%s' % composite\n",
    "    last_composite = 'last_%s' % composite\n",
    "    \n",
    "    # adjust the GLM model\n",
    "    glm_out = sm.GLM(\n",
    "        normative_subjects[last_composite], Xvar,\n",
    "        family=sm.families.Gaussian()\n",
    "    ).fit()\n",
    "    \n",
    "    # display summary\n",
    "    print(glm_out.summary())\n",
    "    print('----- Initial correlation\\n')\n",
    "    display(\n",
    "        normative_subjects[[last_composite, 'last_age', 'last_yschooling']].corr().round(decimals=3)\n",
    "    )\n",
    "    \n",
    "    # plot initial composite distribution\n",
    "    fig, ax = plt.subplots(figsize=(5, 3))\n",
    "    ax.hist(neuro[composite].values, label='Raw', alpha=0.3)\n",
    "    \n",
    "    # make the adjustment\n",
    "    neuro[composite] -= (\n",
    "        (neuro[['age', 'yschooling']] - Xvar_mean.values) * \n",
    "        glm_out.params[['last_age', 'last_yschooling']].values\n",
    "    ).sum(axis=1)\n",
    "    \n",
    "    # plot the adjusted distribution\n",
    "    ax.hist(neuro[composite].values, label='Adj', alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Freq')\n",
    "    ax.set_title(composite)\n",
    "    plt.show()\n",
    "    \n",
    "    print('----- Final correlation\\n')\n",
    "    last_normative_neuro_adjusted = neuro.loc[\n",
    "        normative_subjects.index.get_level_values('subject_id')\n",
    "    ].copy().groupby('subject_id').nth(-1)\n",
    "    display(\n",
    "        last_normative_neuro_adjusted[[composite, 'age', 'yschooling']].corr().round(decimals=3)\n",
    "    )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale composite values as with the centiloid scale considering 0 the value of the 50th percentile of controls\n",
    "# and the as 1 the 50th percentile of the dementia samples mild dementia\n",
    "# create the diagnosis masks\n",
    "dementia_mask = (\n",
    "    (neuro.diagnosis == 'dementia') & \n",
    "    (neuro.dementia_stage == 'mild')\n",
    ")\n",
    "# adjust composites\n",
    "for composite in composites.columns:\n",
    "    # select the lower and upper bounds\n",
    "    lower_val = neuro.loc[dementia_mask][composite].quantile(0.5)\n",
    "    upper_val = neuro.loc[normative_subjects.index.unique()][composite].quantile(0.5)\n",
    "    \n",
    "    # scale the values\n",
    "    neuro[composite] = ((neuro[composite] - lower_val) / (upper_val - lower_val)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display intra-composite calculation\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    data=neuro[composites.columns.tolist()].corr(),\n",
    "    annot=True, fmt=\".2f\",\n",
    "    vmin=0, vmax=1,\n",
    "    cmap='Blues'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 11))\n",
    "sns.heatmap(\n",
    "    data=neuro[composite_source_vars + composites.columns.tolist()].corr(),\n",
    "    annot=True, fmt=\".2f\",\n",
    "    vmin=0, vmax=1,\n",
    "    cmap='Spectral_r'\n",
    ")\n",
    "# group test by cognitive domain\n",
    "groups = {}\n",
    "i = 0\n",
    "for domain, tests in SEM_STRUCTURE.items():\n",
    "    groups[domain] = (i, i+len(tests)-1)\n",
    "    i += len(tests)\n",
    "    \n",
    "for group, (start, end) in groups.items():\n",
    "    # Añadir un rectángulo alrededor del grupo\n",
    "    rect = plt.Rectangle((start, start), end-start+1, end-start+1, fill=False, edgecolor='black', lw=2)\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the explained variance of each test\n",
    "composite_ev = []\n",
    "for domain, tests in SEM_STRUCTURE.items():\n",
    "    for test in tests:\n",
    "        x_var = neuro[f'{domain}_composite'].values[:, np.newaxis]\n",
    "        y_var = neuro[test].values\n",
    "        y_var_hat = LinearRegression(fit_intercept=True).fit(x_var, y_var).predict(x_var)\n",
    "        ev = explained_variance_score(y_var, y_var_hat)\n",
    "        composite_ev.append({'domain': domain, 'test': test, 'ev': ev})\n",
    "composite_ev = pd.DataFrame(composite_ev)\n",
    "\n",
    "composite_ev.set_index(['domain', 'test']).sort_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before but for test that dont belong to the composite\n",
    "composite_no_belong_ev = []\n",
    "for domain, tests in SEM_STRUCTURE.items():\n",
    "    for test in composite_source_vars:\n",
    "        if test in tests: continue\n",
    "\n",
    "        x_var = neuro[f'{domain}_composite'].values[:, np.newaxis]\n",
    "        y_var = neuro[test].values\n",
    "        y_var_hat = LinearRegression(fit_intercept=True).fit(x_var, y_var).predict(x_var)\n",
    "        ev = explained_variance_score(y_var, y_var_hat)\n",
    "        composite_no_belong_ev.append({'domain': domain, 'test': test, 'ev': ev})\n",
    "        \n",
    "composite_no_belong_ev = pd.DataFrame(composite_no_belong_ev)\n",
    "\n",
    "composite_no_belong_ev.set_index(['domain', 'test']).sort_index().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # display composites distribution\n",
    "    n_subplots = len(composites.columns)\n",
    "    num_rows = (n_subplots + 1) // 2\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(11, 3.5 * num_rows))\n",
    "    axes = axes.flatten() if num_rows > 0 else [axes]\n",
    "\n",
    "    for ax, composite in zip(axes, composites.columns):\n",
    "        ax = sns.histplot(\n",
    "            data=neuro,\n",
    "            x=composite,\n",
    "            hue='diagnosis',\n",
    "            # kde=True,\n",
    "            hue_order=['control', 'mci', 'dementia'],\n",
    "            palette=['#27AE60', '#F5B041', '#C0392B'],\n",
    "            stat='density',\n",
    "            common_norm=False,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_title(composite.replace('_composite', ''), size=15)\n",
    "        ax.grid(alpha=0.2, color='black')\n",
    "        ax.set_xlabel('Value')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set minimum and maximum values to the range [-200, 200]\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # display composites distribution\n",
    "    n_subplots = len(composites.columns)\n",
    "    num_rows = (n_subplots + 1) // 2\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(11, 3.5 * num_rows))\n",
    "    axes = axes.flatten() if num_rows > 0 else [axes]\n",
    "\n",
    "    for ax, composite in zip(axes, composites.columns):\n",
    "        neuro.loc[neuro[composite] < -200, composite] = -200\n",
    "        neuro.loc[neuro[composite] > 200, composite] = 200\n",
    "        ax = sns.histplot(\n",
    "            data=neuro,\n",
    "            x=composite,\n",
    "            hue='diagnosis',\n",
    "            # kde=True,\n",
    "            hue_order=['control', 'mci', 'dementia'],\n",
    "            palette=['#27AE60', '#F5B041', '#C0392B'],\n",
    "            stat='density',\n",
    "            common_norm=False,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_title(composite.replace('_composite', ''), size=15)\n",
    "        ax.grid(alpha=0.2, color='black')\n",
    "        ax.set_xlabel('Value')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the generated data\n",
    "neuro.to_parquet(\n",
    "    PATH_TO_STUDY_DATA / ('%s_neuropsycho_imputed_composites_v1.0.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
