{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook used to calculate the brain connectivity matrix based on FDG-PET baseline data used as input for the graph neural networks. The connectivity graph will be estimated based on Gaussian graphical models (Graphical Lasso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.covariance import GraphicalLasso\n",
    "from IPython.display import display\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base data directory\n",
    "BASE_DATA_PATH = Path(os.path.join('..',  'data'))\n",
    "\n",
    "# FDG-PET data\n",
    "PATH_TO_FDG_DATA = BASE_DATA_PATH / 'mounts' / 'v1' / '20240428_fdg_cross_sectional.parquet'\n",
    "\n",
    "# path to split information used to prevent information about test data from being leaked in the calculation \n",
    "# of connectivity networks\n",
    "PATH_TO_SPLIT_INFO = BASE_DATA_PATH / 'splits' / 'v1' / '20240428_longitudinal_generated20240428.parquet'\n",
    "\n",
    "# directory where the generated connectivity matrices will be exported to\n",
    "PATH_TO_OUTPUT = BASE_DATA_PATH / 'graphical_lasso' / 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostrapGraphicalLasso(\n",
    "    X: pd.DataFrame,\n",
    "    alphas: Union[list, np.ndarray],\n",
    "    niter: int,\n",
    "    n_samples: int,\n",
    "    replacement: bool,\n",
    "    maxiter: int,\n",
    "    n_jobs: int = 1\n",
    "):\n",
    "    \"\"\" Subroutine used to fit the Graphical Lasso model by applying boostrap to extract \n",
    "    the BIC value and a stability estimate of the selected edges.\n",
    "    \"\"\"\n",
    "    def __worker__(\n",
    "        _X: pd.DataFrame,\n",
    "        _alpha: float,\n",
    "        _niter: int,\n",
    "        _n_samples: int,\n",
    "        _replacement: bool,\n",
    "        _maxiter: int\n",
    "    ):\n",
    "        precisions = []\n",
    "        bics = []\n",
    "        loglike = []\n",
    "        determinants = []\n",
    "        pen_term = []\n",
    "        eigenvalues = []\n",
    "        condition = []\n",
    "        n_edges = []\n",
    "        curr_iter_ = 0\n",
    "        with tqdm(total=_niter) as pbar:\n",
    "            while len(bics) < _niter:\n",
    "                # generate the sample data\n",
    "                X_sample = _X.iloc[np.random.choice(range(len(_X)), size=_n_samples, replace=_replacement)]\n",
    "\n",
    "                try:\n",
    "                    with warnings.catch_warnings(record=True) as w:\n",
    "                        warnings.simplefilter(\"always\")\n",
    "                        glasso = GraphicalLasso(\n",
    "                            alpha=_alpha,\n",
    "                            mode='cd',\n",
    "                            covariance='precomputed',\n",
    "                            max_iter=100,\n",
    "                            assume_centered=True,\n",
    "                        ).fit(X_sample.cov())\n",
    "                            \n",
    "                except Exception as ex:\n",
    "                    curr_iter_ += 1\n",
    "                    if curr_iter_ > _maxiter:\n",
    "                        print('(1) Exception (alpha={}): {}'.format(_alpha, ex))\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                # calculate the log likelihood of the data (positive better)\n",
    "                # Sklearn assumes that the sample size is constant and does not include \n",
    "                # it in the equation. For this reason it is necessary to multiply by N\n",
    "                log_likelihood = glasso.score(X_sample) * len(X_sample)\n",
    "                if np.isinf(log_likelihood):\n",
    "                    curr_iter_ += 1\n",
    "                    if curr_iter_ > _maxiter:\n",
    "                        print('(2) Maximum number of iterations reached (alpha={})'.format(_alpha))\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # save the log-likelihood\n",
    "                loglike.append(log_likelihood)\n",
    "\n",
    "                # save the determinant of the precision matrix\n",
    "                determinants.append(\n",
    "                    np.linalg.det(glasso.precision_)\n",
    "                )\n",
    "                \n",
    "                # save the penalization term of the objective function\n",
    "                pen_term.append(\n",
    "                    np.sum(np.abs(glasso.precision_))\n",
    "                )\n",
    "\n",
    "                # save the eigenvalues\n",
    "                eigenvalues.append(\n",
    "                    np.linalg.eig(glasso.precision_)[0]\n",
    "                )\n",
    "\n",
    "                # save the condition number (euclidean norm(\n",
    "                condition.append(\n",
    "                    np.linalg.cond(glasso.precision_, p=None)\n",
    "                )\n",
    "\n",
    "                # get the precision matrix\n",
    "                precision = glasso.precision_\n",
    "                \n",
    "                # binarize the precision matrix\n",
    "                zero_mask = np.isclose(precision, 0)\n",
    "                precision[np.where(zero_mask)] = 0\n",
    "                precision[np.where(~zero_mask)] = 1\n",
    "                \n",
    "                # calculate the number of parameters\n",
    "                num_params = np.sum(np.triu(precision != 0, k=0))\n",
    "\n",
    "                # get the number of edges\n",
    "                n_edges.append(num_params - len(precision))   # off-diagonal elements\n",
    "                \n",
    "                # calculate the bic (inverting the sign to match sklearn implementation framework)\n",
    "                bic = 2 * log_likelihood - num_params * np.log(len(X_sample))\n",
    "\n",
    "                # save the precision matrix and bic\n",
    "                precisions.append(precision)\n",
    "                bics.append(bic)\n",
    "            \n",
    "                curr_iter_ += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                if curr_iter_ > _maxiter:\n",
    "                    print('(3) Maximum number of iterations reached (alpha={})'.format(_alpha))\n",
    "                    break\n",
    "            \n",
    "        # get the mean precision matrix\n",
    "        mean_precision = np.stack(precisions).mean(axis=0)\n",
    "    \n",
    "        return {\n",
    "            'alpha': _alpha,\n",
    "            'mean_precision': mean_precision,\n",
    "            'bic': np.array(bics),\n",
    "            'll': np.array(loglike),\n",
    "            'det': np.array(determinants),\n",
    "            'l_1': np.array(pen_term),\n",
    "            'eigen': np.stack(eigenvalues),\n",
    "            'n_edges': np.array(n_edges),\n",
    "            'condition': np.array(condition)\n",
    "            \n",
    "        }\n",
    "\n",
    "    if n_jobs == 1:\n",
    "        glasso_out = [\n",
    "            __worker__(\n",
    "                _X=X,\n",
    "                _alpha=alpha,\n",
    "                _niter=niter,\n",
    "                _n_samples=n_samples,\n",
    "                _replacement=replacement,\n",
    "                _maxiter=maxiter\n",
    "            ) for alpha in alphas\n",
    "        ]\n",
    "    else:\n",
    "        glasso_out = joblib.Parallel(backend='loky', n_jobs=n_jobs)(\n",
    "            joblib.delayed(__worker__)(\n",
    "                _X=X,\n",
    "                _alpha=alpha,\n",
    "                _niter=niter,\n",
    "                _n_samples=n_samples,\n",
    "                _replacement=replacement,\n",
    "                _maxiter=maxiter\n",
    "            ) for alpha in alphas\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        'alpha': np.array([e['alpha'] for e in glasso_out]), \n",
    "        'bic': np.array([e['bic'] for e in glasso_out]),\n",
    "        'll': np.array([e['ll'] for e in glasso_out]),\n",
    "        'det': np.array([e['det'] for e in glasso_out]),\n",
    "        'l_1': np.array([e['l_1'] for e in glasso_out]),\n",
    "        'eigen': np.array([e['eigen'] for e in glasso_out]),\n",
    "        'n_edges': np.array([e['n_edges'] for e in glasso_out]),\n",
    "        'condition': np.array([e['condition'] for e in glasso_out]),\n",
    "        'mean_precision': np.array([e['mean_precision'] for e in glasso_out])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input data and select the variables of intereset for the target population\n",
    "df = pd.read_parquet(PATH_TO_FDG_DATA)\n",
    "split_df = pd.read_parquet(PATH_TO_SPLIT_INFO)\n",
    "print(f'Data shape (initial): {df.shape[0]}')\n",
    "\n",
    "# prevent possible data leak from the test data\n",
    "df = df.loc[\n",
    "  ~df.index.get_level_values('subject_id').isin(\n",
    "      split_df.loc[split_df['split'] == 'test'].index.get_level_values('subject_id').values\n",
    "  ) \n",
    "].copy()\n",
    "\n",
    "# remove dementia subjects\n",
    "df = df.loc[\n",
    "    df['diagnosis'].isin(['control', 'mci', 'dementia'])\n",
    "].copy()\n",
    "print(f'Data shape (no dementia): {df.shape[0]}')\n",
    "\n",
    "# select baseline information\n",
    "df = df.groupby('subject_id').nth(0)\n",
    "print(f'Data shape (baseline): {df.shape[0]}')\n",
    "\n",
    "# save diagnostic information\n",
    "diag_df = df[['diagnosis']].copy()\n",
    "\n",
    "# select average FDG SUVR values removing vermis and cerebellum\n",
    "df = df[[\n",
    "    c for c in df.columns if re.match('^aal_pet_fdg.*mean', c)\n",
    "  ]].copy()\n",
    "\n",
    "print(f'Data shape (variable selection): {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the input data\n",
    "X = df.copy()\n",
    "\n",
    "th = 1.2   # HACK. Avoid creating meta-rois\n",
    "rois_to_merge = []\n",
    "correlations = []\n",
    "X_meta = X.copy()\n",
    "for i in range(X.shape[1]):\n",
    "    for j in range(i+1, X.shape[1]):\n",
    "        corr = X.iloc[:, [i, j]].corr().iloc[0, 1]\n",
    "        correlations.append(corr)\n",
    "        if abs(corr) > th:\n",
    "            # display the information\n",
    "            print(list(X.iloc[:, [i, j]].columns), round(corr, 3))\n",
    "\n",
    "            # select the names\n",
    "            r1, r2 = X.columns[i], X.columns[j]\n",
    "\n",
    "            # save the ROIs\n",
    "            rois_to_merge.append((r1, r2))\n",
    "\n",
    "# create the meta-ROI\n",
    "if len(rois_to_merge) > 0:\n",
    "    # create a graph to explore the relationships\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(rois_to_merge)\n",
    "    \n",
    "    # get the subgraphs\n",
    "    subgraphs = list(nx.connected_components(G))\n",
    "    \n",
    "    # display the graph\n",
    "    nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=8)\n",
    "    plt.show()\n",
    "    \n",
    "    subgraph_hash = {}\n",
    "    for idx, elements in enumerate(subgraphs):\n",
    "        subgraph_hash[idx] = elements\n",
    "        X_meta['META_%d' % idx] = X_meta[list(elements)].mean(axis=1)\n",
    "        X_meta = X_meta.drop(columns=list(elements))\n",
    "\n",
    "# standarize the data \n",
    "X_meta = (X_meta - X_meta.mean()) / X_meta.std()\n",
    "X_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the correlation between brain regions\n",
    "fig, ax = plt.subplots(figsize=(6, 3.5))\n",
    "ax.hist(correlations, bins=30)\n",
    "ax.set_title('Correlations', size=15)\n",
    "ax.grid(alpha=0.2, color='black')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(\n",
    "    X_meta.cov(),\n",
    "    vmin=-1, vmax=1,\n",
    "    cmap='Spectral_r',\n",
    "    ax=ax,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapping analysis of lambda values\n",
    "output = boostrapGraphicalLasso(\n",
    "    X=X_meta,\n",
    "    alphas=np.linspace(0.1, 0.8, 100),\n",
    "    niter=500,\n",
    "    n_samples=int(len(X) * 0.75),\n",
    "    replacement=False,\n",
    "    maxiter=1000,\n",
    "    n_jobs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "axes[0].plot(\n",
    "    output['alpha'],    \n",
    "    output['condition'].mean(axis=1),\n",
    ")\n",
    "axes[0].set_title(r'Mean cond (euc)', size=14)\n",
    "\n",
    "axes[1].plot(\n",
    "    output['alpha'],    \n",
    "    output['condition'].std(axis=1),\n",
    ")\n",
    "axes[1].set_title(r'Std cond (euc)', size=14)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    for pos in ['top', 'bottom', 'right', 'left']:\n",
    "        ax.spines[pos].set_visible(False)\n",
    "    ax.grid(alpha=0.3, color='black')\n",
    "    ax.set_xlabel('$\\lambda$')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "axes[0].plot(\n",
    "    output['alpha'],    \n",
    "    np.log(output['det']).mean(axis=1),\n",
    ")\n",
    "axes[0].set_title(r'Mean $log(|\\Theta|)$', size=14)\n",
    "\n",
    "axes[1].plot(\n",
    "    output['alpha'],    \n",
    "    np.log(output['det']).std(axis=1),\n",
    ")\n",
    "axes[1].set_title(r'Std $log(|\\Theta|)$', size=14)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    for pos in ['top', 'bottom', 'right', 'left']:\n",
    "        ax.spines[pos].set_visible(False)\n",
    "    ax.grid(alpha=0.3, color='black')\n",
    "    ax.set_xlabel('$\\lambda$')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "axes[0].plot(\n",
    "    output['alpha'],    \n",
    "    output['n_edges'].mean(axis=1),\n",
    ")\n",
    "axes[0].set_title(r'Mean number of edges', size=14)\n",
    "\n",
    "axes[1].plot(\n",
    "    output['alpha'],    \n",
    "    output['n_edges'].std(axis=1),\n",
    ")\n",
    "axes[1].set_title(r'Std number of edges', size=14)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    for pos in ['top', 'bottom', 'right', 'left']:\n",
    "        ax.spines[pos].set_visible(False)\n",
    "    ax.grid(alpha=0.3, color='black')\n",
    "    ax.set_xlabel('$\\lambda$')\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the implementation of sklearn higher values of log-likelihodd are better\n",
    "#\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.plot(\n",
    "    output['alpha'],    \n",
    "    output['bic'].mean(axis=1),\n",
    "    lw=2,\n",
    "    label='BIC'\n",
    ")\n",
    "ax.plot(\n",
    "    output['alpha'],    \n",
    "    output['ll'].mean(axis=1),\n",
    "    lw=2,\n",
    "    label='LL'\n",
    ")\n",
    "ax.plot(\n",
    "    output['alpha'],    \n",
    "    (2*output['ll'] - output['n_edges'] * np.log(int(len(X) * 0.75)) - \n",
    "     4 * output['n_edges'] * np.log(X_meta.shape[1]) * .5).mean(axis=1),\n",
    "    lw=2,\n",
    "    label='eBIC ($\\gamma=0.5$)'\n",
    ")\n",
    "\n",
    "for pos in ['top', 'bottom', 'right', 'left']:\n",
    "    ax.spines[pos].set_visible(False)\n",
    "ax.grid(alpha=0.3, color='black')\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "    \n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# optimal lambda according to eBIC\n",
    "output['alpha'][np.argmax(\n",
    "    (2*output['ll'] - output['n_edges'] * np.log(int(len(X) * 0.75)) - \n",
    "     4 * output['n_edges'] * np.log(X_meta.shape[1]) * .5).mean(axis=1)\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit various graphical lasso models for different values of lambda selected on the basis of the above analyses\n",
    "lambda_values = [0.1, 0.2, 0.3, 0.4]\n",
    "final_conn_matrices = {}\n",
    "for lambda_ in lambda_values:\n",
    "    print(f'----- Processing lambda: {lambda_:.3f}')\n",
    "\n",
    "    # create and fit the final glasso model using all available data\n",
    "    final_glasso = GraphicalLasso(\n",
    "        alpha=lambda_,\n",
    "        mode='cd',\n",
    "        covariance='precomputed',\n",
    "        max_iter=500,\n",
    "        assume_centered=True,\n",
    "    ).fit(X_meta.cov())\n",
    "\n",
    "    print('Model log-likelihood: {:.2f}'.format(final_glasso.score(X_meta) * len(X_meta)))\n",
    "    print('Precision matrix determinant: {:.2f}'.format(np.linalg.det(final_glasso.precision_)))\n",
    "    print('Number of edges: {:.0f}'.format((~np.isclose(final_glasso.precision_, 0.0)).sum() / 2 - X_meta.shape[1]))\n",
    "\n",
    "    # create the final connectivity matrix\n",
    "    final_conn = pd.DataFrame(\n",
    "        (~np.isclose(final_glasso.precision_, 0)).astype(int),\n",
    "        columns=X_meta.columns,\n",
    "        index=X_meta.columns)\n",
    "\n",
    "    # add the ROIs that were grouped into Meta-ROIs\n",
    "    if len(rois_to_merge) > 0:\n",
    "        for k, rois in subgraph_hash.items():\n",
    "            meta_roi = 'META_%d' % k\n",
    "            for roi in rois:\n",
    "                final_conn.loc[roi] = final_conn.loc[meta_roi]\n",
    "                final_conn[roi] = final_conn[meta_roi]\n",
    "        \n",
    "            # remove meta ROI\n",
    "            final_conn = final_conn.drop(index=[meta_roi], columns=[meta_roi])\n",
    "        \n",
    "            # add connections between ROIs\n",
    "            final_conn.loc[list(rois), list(rois)] = 1\n",
    "        \n",
    "    # remove diagonal elements\n",
    "    for i in range(len(final_conn)):\n",
    "        final_conn.iloc[i, i] = 0\n",
    "\n",
    "    # fix variable names\n",
    "    var_hash = {\n",
    "        k: k.replace('aal_pet_fdg_', '').replace('_mean', '') \n",
    "        for k in final_conn.columns}\n",
    "    final_conn.columns = [var_hash[c] for c in final_conn.columns]\n",
    "    final_conn.index = [var_hash[c] for c in final_conn.index]\n",
    "\n",
    "    # check if the remaining graph is connected\n",
    "    G = nx.from_pandas_adjacency(final_conn)\n",
    "    assert nx.is_connected(G),\\\n",
    "        f'Graph associated to lambda {lambda_:.3f} contains unconnected nodes.'\n",
    "\n",
    "    # calculate some graph theory metrics\n",
    "    graph_metrics = pd.concat([\n",
    "        pd.DataFrame([nx.degree_centrality(G)], index=['Centrality']).T,\n",
    "        pd.DataFrame([nx.eigenvector_centrality(G)], index=['Eigenvector centrality']).T,\n",
    "        pd.DataFrame([nx.betweenness_centrality(G)], index=['Betweenness centrality']).T,\n",
    "        pd.DataFrame([nx.average_neighbor_degree(G)], index=['Neighboor degree']).T\n",
    "    ], axis=1)\n",
    "\n",
    "    # display the graph metrics for the top 10\n",
    "    gmetric_df_head = []\n",
    "    for gmetric in [\n",
    "        'Centrality', 'Eigenvector centrality', 'Betweenness centrality', 'Neighboor degree'\n",
    "    ]:\n",
    "        sub_df_ = graph_metrics.sort_values(by=gmetric, ascending=False)[[gmetric]].head(10)\n",
    "        sub_df_.index.names = [f'ROI- {gmetric}']\n",
    "        sub_df_ = sub_df_.reset_index()\n",
    "        gmetric_df_head.append(sub_df_)\n",
    "        \n",
    "    display(pd.concat(gmetric_df_head, axis=1).round(decimals=3))\n",
    "\n",
    "    # save the connectivity matrix \n",
    "    final_conn_matrices[lambda_] = final_conn\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the connectivity matrices\n",
    "curr_date = datetime.now().strftime('%Y%m%d')\n",
    "for lambda_, conn_matrix in final_conn_matrices.items():\n",
    "    conn_matrix.to_parquet(\n",
    "        PATH_TO_OUTPUT / f'{curr_date}_lambda_{lambda_:.3f}.parquet'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AD-GNN-v4",
   "language": "python",
   "name": "ad-gnn-v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
