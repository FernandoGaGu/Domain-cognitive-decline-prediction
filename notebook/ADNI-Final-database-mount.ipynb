{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook used to build the final databases used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base path where the neuropsychological data is located\n",
    "BASE_STUDY_PATH = Path(os.path.join('..', 'data'))\n",
    "\n",
    "# path to the calculated composites\n",
    "PATH_TO_COMPOSITES  = BASE_STUDY_PATH / 'adni' / 'processed' / '20240428_neuropsycho_imputed_composites_v1.0.parquet'\n",
    "\n",
    "# path to the extracted image features and detected outliers\n",
    "PATH_TO_IMAGE_FEATS = {\n",
    "    'mri': {\n",
    "        'data': BASE_STUDY_PATH / 'neuroimaging' / 'v1' / '20240428_MRI_intermodality_v0.parquet',\n",
    "        'outliers': BASE_STUDY_PATH / 'neuroimaging' / 'v1' / '20240428_MRI_intermodality_v0_outliers_generated20240428.parquet'\n",
    "    },\n",
    "    'fdg': {\n",
    "        'data': BASE_STUDY_PATH / 'neuroimaging' / 'v1' / '20240428_FDG_intermodality_v0.parquet',\n",
    "        'outliers': BASE_STUDY_PATH / 'neuroimaging' / 'v1' / '20240428_FDG_intermodality_v0_outliers_generated20240428.parquet'\n",
    "    },\n",
    "    'amy': {\n",
    "        'data': BASE_STUDY_PATH / 'neuroimaging' / 'v1' / '20240428_AMY_intermodality_v0.parquet',\n",
    "        'outliers': BASE_STUDY_PATH / 'neuroimaging' / 'v1' / '20240428_AMY_intermodality_v0_outliers_generated20240428.parquet'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Minimum separation (in years) between two images belonging to the same subject to be considered as two different\n",
    "# trajectories\n",
    "NEUROIMAGING_ACQ_OFFSET = 2\n",
    "\n",
    "# minimum and maximum number of allowed follow-ups (limits are included)\n",
    "MIN_NUMBER_FOLLOWUPS = 2\n",
    "MAX_NUMBER_FOLLOWUPS = 8\n",
    "\n",
    "# minimum and maximum number of allowed years of follow-up (limits are included)\n",
    "MIN_YEARS_FOLOWUP = 2\n",
    "MAX_YEARS_FOLLOWUP = 4\n",
    "\n",
    "# percentile based on the healthy controls used to establish a maximum value that a given neuropsychological\n",
    "# composite can has. This pretends to reduce the variability in the subject-level trajectories\n",
    "PERCENTILE_FOR_HC_CLIPPING = 0.5\n",
    "\n",
    "# allowed window between modalities (in days)\n",
    "CROSS_NEUROIMAGING_WINDOW = 30.5 * 3  # +/- 3 months\n",
    "\n",
    "# allowed window between neuroimaging and neuropsychological data (in days)\n",
    "CROSS_NEUROPSYCHO_WINDOW = 30.5 * 3  # +/- 3 months\n",
    "\n",
    "# variables used to calculate the trayectories\n",
    "TRAJECTORY_VARIABLES = [\n",
    "    'memory_composite',\n",
    "    'exec_composite',\n",
    "    'language_composite',\n",
    "    'visuospatial_composite'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateTimeSeries(\n",
    "    t: np.ndarray,\n",
    "    y: np.ndarray, \n",
    "    step: float,\n",
    "    max_t: float\n",
    ") -> tuple:\n",
    "    \"\"\" Subroutine used to interpolate the input time series to regular\n",
    "    sample points \"\"\"\n",
    "    \n",
    "    # fit the split\n",
    "    spl = PchipInterpolator(t, y)\n",
    "    \n",
    "    # interpolate time points using a step interval until max_t \n",
    "    sim_t = np.arange(0, max_t, step)\n",
    "    sim_y = spl(sim_t)\n",
    "    \n",
    "    # select the last time point to restrict the extrapolation\n",
    "    last_idx = np.argmin((np.abs(sim_t - t[-1]) - step)) + 1\n",
    "    \n",
    "    # select values\n",
    "    sim_y = sim_y[:last_idx]\n",
    "    sim_t = sim_t[:last_idx]\n",
    "\n",
    "    return sim_t, sim_y\n",
    "\n",
    "\n",
    "def setLayout(ax):\n",
    "    \"\"\" Helping function to set axes layout \"\"\"\n",
    "    for pos in ['top', 'bottom', 'right', 'left']:\n",
    "        ax.spines[pos].set_visible(False)\n",
    "    ax.grid(alpha=0.3, color='black')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def addYears(date, years):\n",
    "    \"\"\" Helping function used to add years using float values \"\"\"\n",
    "    return date + pd.DateOffset(years=int(years), months=int((years % 1) * 12))\n",
    "\n",
    "\n",
    "def adjustTrajectory(x: np.ndarray, y: np.ndarray, min_change: float, max_change: float) -> np.ndarray:\n",
    "    \"\"\" Subroutine used to stabilize Neuropsychology score trajectories by setting a \n",
    "    maximum and minimum allowable change. \"\"\"\n",
    "\n",
    "    y = deepcopy(y)\n",
    "    \n",
    "    # If from the first evaluation to the minimum of the next two evaluations there is an increase of more than \n",
    "    # 50 points in absolute value, we assume that the first evaluation was incorrect and assign the average value \n",
    "    # of the next two evaluations. This is intended to reduce the effects of “fast retrievers” due to artifacts.\n",
    "    # Apply this logic to all evaluations\n",
    "    if len(y) >= 3:\n",
    "        for i in range(len(y) - 2):\n",
    "            if (y[i] - y[i+1:i+3].min()) < -50:\n",
    "                y[i] = y[i+1:i+3].mean()\n",
    "\n",
    "    # then clip allowable changes\n",
    "    stable = False\n",
    "    while not stable:\n",
    "        # calculate the deltas\n",
    "        delta_y = np.diff(y, prepend=y[0])\n",
    "        delta_norm = delta_y / np.diff(x, prepend=1)\n",
    "        \n",
    "        # adjust the changes\n",
    "        delta_norm = np.array([min(max(v, min_change), max_change) for v in delta_norm])\n",
    "        \n",
    "        # apply the inverse transform\n",
    "        y_adj = delta_norm * np.diff(x, prepend=0) + np.append(y[0], y[:-1])\n",
    "        \n",
    "        # the trajectory is stable\n",
    "        stable = np.sum(np.abs(y_adj - y)) < 0.1\n",
    "    \n",
    "        # update y variable\n",
    "        y = y_adj\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def smoothTrajectory(t: np.ndarray, y: np.ndarray, int_freq: float) -> tuple:\n",
    "    \"\"\" Smooth the input trajectory by appying a moving LR model. \"\"\"\n",
    "    def _getLRAdjTraj(_x: np.ndarray, _y: np.ndarray, window: int) -> np.ndarray:\n",
    "        # get linear regerssion predictions\n",
    "        preds = []\n",
    "        for t in range(window, len(_x)+1):\n",
    "            min_t = t-window\n",
    "            max_t = t\n",
    "\n",
    "            # fit a linear regression model\n",
    "            lr = LinearRegression(fit_intercept=True).fit(_x[min_t:max_t, np.newaxis], _y[min_t:max_t])\n",
    "            preds.append(lr.predict(_x[min_t:max_t, np.newaxis]))\n",
    "        # average predictions considering the time stamp\n",
    "        preds_t = np.full((len(preds), len(preds[0]) + len(preds)-1), np.nan)\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            preds_t[i, i:i+len(preds[0])] = preds[i]\n",
    "\n",
    "        return np.nanmean(preds_t, axis=0)\n",
    "    \n",
    "    # interpolate the time series\n",
    "    int_t, int_y = interpolateTimeSeries(t, y, step=int_freq, max_t=np.max(t)+0.05)\n",
    "\n",
    "    if len(int_t) >= 6:   # at least 6 time points\n",
    "        # apply a moving window of size 4\n",
    "        return int_t, _getLRAdjTraj(int_t, int_y, 4)\n",
    "    elif len(int_t) >= 5:  # at lest 5 time points\n",
    "        # apply a moving window of size 3\n",
    "        return int_t, _getLRAdjTraj(int_t, int_y, 3)\n",
    "    elif len(int_t) >= 3:\n",
    "        # return the interpolated values\n",
    "        return int_t, int_y\n",
    "    else:\n",
    "        # return the origin values\n",
    "        return t, y\n",
    "    \n",
    "\n",
    "def crossNearestInfo(\n",
    "    target_df: pd.DataFrame,\n",
    "    source_df: pd.DataFrame,\n",
    "    how: str,\n",
    "    window: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Function used to cross dataframes by nearest date \"\"\"\n",
    "    days_diff_colname = '_days_diff'\n",
    "\n",
    "    # get date columns\n",
    "    subject_id = target_df.index.names[0]\n",
    "    target_date = target_df.index.names[1]\n",
    "    source_date = source_df.index.names[1]\n",
    "    \n",
    "    # cross information\n",
    "    crossed_df = target_df.join(source_df, how=how).reset_index()\n",
    "\n",
    "    # select information in window\n",
    "    crossed_df = crossed_df.loc[(crossed_df[target_date] - crossed_df[source_date]).dt.days.abs() < window].copy()\n",
    "\n",
    "    # select the nearest date\n",
    "    crossed_df[days_diff_colname] = (crossed_df[target_date] - crossed_df[source_date]).dt.days.abs()\n",
    "    \n",
    "    # remove possible duplicates\n",
    "    crossed_df = crossed_df.reset_index().set_index([subject_id, target_date, days_diff_colname]).sort_index()\n",
    "    crossed_df = crossed_df.groupby([subject_id, target_date]).nth(0).reset_index(days_diff_colname)\n",
    "    \n",
    "    assert not crossed_df.index.duplicated().any()\n",
    "\n",
    "    crossed_df = crossed_df.drop(columns=[days_diff_colname, 'index'])\n",
    "\n",
    "    return crossed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroimaging information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the neuroimaging information\n",
    "cross_mod_data = {}\n",
    "for mod, mod_files in PATH_TO_IMAGE_FEATS.items():\n",
    "    # load the data and the outliers\n",
    "    data = pd.read_parquet(mod_files['data'])\n",
    "    outliers = pd.read_parquet(mod_files['outliers'])\n",
    "\n",
    "    print('(%s) Init shape: %r' % (mod, list(data.shape)))\n",
    "\n",
    "    # remove outliers\n",
    "    data = data.loc[outliers.loc[outliers.outlier == 0].index].copy()\n",
    "    \n",
    "    print('(%s) Shape after removing outliers: %r' % (mod, list(data.shape)))\n",
    "\n",
    "    # for different acquisitions performed in the same day select the last one\n",
    "    data = data\\\n",
    "        .sort_index().reset_index('acquisition_id').drop(columns=['acquisition_id'])\\\n",
    "        .groupby(['subject_id', 'date']).nth(-1).copy()\n",
    "    \n",
    "    print('(%s) Shape after removing duplicated acquisitions: %r\\n' % (mod, list(data.shape)))\n",
    "    \n",
    "    cross_mod_data[mod] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a crossed dataset with all the data built upon the MRI \n",
    "mri_fdg = crossNearestInfo(\n",
    "    target_df=cross_mod_data['mri'],\n",
    "    source_df=cross_mod_data['fdg'],\n",
    "    how='inner',\n",
    "    window=CROSS_NEUROIMAGING_WINDOW\n",
    ")\n",
    "\n",
    "print('Number of entries MRI-FDG merging: %d' % len(mri_fdg))\n",
    "\n",
    "mri_fdg_amy = crossNearestInfo(\n",
    "    target_df=mri_fdg,\n",
    "    source_df=cross_mod_data['amy'],\n",
    "    how='inner',\n",
    "    window=CROSS_NEUROIMAGING_WINDOW\n",
    ")\n",
    "\n",
    "print('Number of entries MRI-FDG-AMY merging: %d' % len(mri_fdg_amy))\n",
    "\n",
    "assert mri_fdg_amy.shape[1] == (\n",
    "    cross_mod_data['mri'].shape[1] + cross_mod_data['fdg'].shape[1] + cross_mod_data['amy'].shape[1])\n",
    "\n",
    "# select all subject evaluations with at least NEUROIMAGING_ACQ_OFFSET\n",
    "mri_fdg_amy_baseline = []\n",
    "for sub_id, sub_df in mri_fdg_amy.groupby('subject_id'):\n",
    "    if len(sub_df) == 1:\n",
    "        mri_fdg_amy_baseline.append(sub_df)\n",
    "        continue\n",
    "    \n",
    "    # select entries to add\n",
    "    dates = sub_df.reset_index('date')['date']\n",
    "    diff_from_baseline = (dates - dates.iloc[0]).dt.days.values\n",
    "\n",
    "    index_to_add = [0]\n",
    "    while  np.any(diff_from_baseline >= NEUROIMAGING_ACQ_OFFSET*365):\n",
    "        idx = int(np.where(diff_from_baseline >= NEUROIMAGING_ACQ_OFFSET*365)[0][0])\n",
    "        index_to_add.append(idx)\n",
    "        diff_from_baseline -= diff_from_baseline[idx]\n",
    "\n",
    "    # save the entry\n",
    "    mri_fdg_amy_baseline.append(sub_df.iloc[index_to_add])\n",
    "\n",
    "mri_fdg_amy_baseline = pd.concat(mri_fdg_amy_baseline, axis=0)\n",
    "mri_fdg_amy_baseline = mri_fdg_amy_baseline.sort_index().copy()\n",
    "\n",
    "print('Number of entries MRI-FDG-AMY baseline: %d' % len(mri_fdg_amy_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuropsychological information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the composite scores\n",
    "composites = pd.read_parquet(PATH_TO_COMPOSITES)\n",
    "composites = composites.sort_index().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the change normalized by year to limit the minimum and maximum allowable change\n",
    "composites_change = composites.copy()\n",
    "composites_change['neurobat_date_'] = composites_change.index.get_level_values('neurobat_date')\n",
    "composites_change = composites_change[TRAJECTORY_VARIABLES].groupby('subject_id').diff().dropna().join(\n",
    "    (composites_change['neurobat_date_'].diff().dt.days / 365).dropna()\n",
    ")\n",
    "for c in composites_change.columns:\n",
    "    composites_change[c] = composites_change[c] / composites_change['neurobat_date_']\n",
    "composites_change = composites_change.drop(columns=['neurobat_date_'])\n",
    "\n",
    "min_composite_allowed_change = composites_change.quantile(0.2).to_dict()\n",
    "max_composite_allowed_change = composites_change.quantile(0.8).to_dict()\n",
    "\n",
    "min_composite_allowed_change, max_composite_allowed_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth erratic trajectories (potential outliers)\n",
    "for sub_id, sub_df in tqdm(composites.groupby('subject_id')):\n",
    "    if len(sub_df) < 3:\n",
    "        continue\n",
    "\n",
    "    for var in TRAJECTORY_VARIABLES:\n",
    "        \n",
    "        # gets a version where the abrupt changes in data are smoothed out\n",
    "        y_adj = adjustTrajectory(\n",
    "            sub_df['years_followup_neurobat'].values,\n",
    "            sub_df[var].values, \n",
    "            min_change=min_composite_allowed_change[var],\n",
    "            max_change=max_composite_allowed_change[var]\n",
    "        )\n",
    "\n",
    "       # apply smoothing by fitting linear regression models in windows and calculating a weighted average (when the \n",
    "       # number of follow-ups allows). \n",
    "        corr_t, corr_y = smoothTrajectory(\n",
    "            sub_df['years_followup_neurobat'].values, \n",
    "            y_adj, \n",
    "            int_freq=1.0\n",
    "        )\n",
    "        # revert the interpolation to match the real time stamps\n",
    "        corr_y = np.interp(sub_df['years_followup_neurobat'].values, corr_t, corr_y)\n",
    "\n",
    "        # correct the values\n",
    "        composites.loc[sub_id, var] = corr_y\n",
    "\n",
    "        \"\"\"  # Trajectory visualization for debugging\n",
    "        plt.plot(\n",
    "            sub_df['years_followup_neurobat'].values, corr_y, marker='o', label='smooth'\n",
    "        )\n",
    "        plt.plot(\n",
    "            sub_df['years_followup_neurobat'].values,\n",
    "            sub_df[var].values, marker='o', label='real'\n",
    "        )\n",
    "        plt.savefig(f'./temp/{sub_id}.png')\n",
    "        plt.close()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select neuropsychological information + neuroimaging + follow-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select subjects and entries with neuroimaging information and neuropsychological \n",
    "# follow-up information\n",
    "composites_neu_long = composites.copy()\n",
    "composites_neu_long = composites_neu_long.loc[\n",
    "    mri_fdg_amy_baseline.index.get_level_values('subject_id').unique()\n",
    "].copy()\n",
    "\n",
    "# for those subjects with MRI, FDG, and Amyloid, remove all the evaluations\n",
    "# before the first acquisition\n",
    "index_to_remove = []\n",
    "for sub_id, sub_date in mri_fdg_amy_baseline.groupby('subject_id').nth(0).index:\n",
    "\n",
    "    # calculate the minimum date adding an offset\n",
    "    min_date = sub_date - pd.DateOffset(\n",
    "        years=int(CROSS_NEUROPSYCHO_WINDOW / 365),\n",
    "        months=int(((CROSS_NEUROPSYCHO_WINDOW / 365) % 1) * 12))\n",
    "\n",
    "    # get the indices that will be removed\n",
    "    index_ = composites_neu_long.loc[[sub_id]].loc[\n",
    "        composites_neu_long.loc[sub_id].index < min_date\n",
    "    ].index\n",
    "    \n",
    "    # save the indices to remove\n",
    "    index_to_remove.append(index_)\n",
    "\n",
    "print('Initial neuropsychological entries: %d' % len(composites_neu_long))\n",
    "\n",
    "for index_ in index_to_remove:\n",
    "    composites_neu_long = composites_neu_long.drop(index=index_)\n",
    "    \n",
    "print('Neuropsychological entries after removing indices with past data: %d' % len(composites_neu_long))\n",
    "\n",
    "print('Number of patients after removing indices: %d' % len(\n",
    "    composites_neu_long.index.get_level_values('subject_id').unique()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to leverage repeated acquisitions, get trajectory-level information. Also filter entries by inclusion criteria\n",
    "visited_subjects = {}\n",
    "miss_in_removed_indices = []\n",
    "miss_by_num_followups = []\n",
    "miss_by_time_followup = []\n",
    "miss_by_neuropsycho_window = []\n",
    "composites_neu_long_window = []\n",
    "\n",
    "unique_composite_subjects = composites_neu_long.index.get_level_values('subject_id').unique()\n",
    "for sub_id, sub_date in mri_fdg_amy_baseline.index:\n",
    "\n",
    "    # apply the same offset than in the previous code to select the evaluations\n",
    "    init_date = sub_date - pd.DateOffset(\n",
    "        years=int(CROSS_NEUROPSYCHO_WINDOW / 365),\n",
    "        months=int(((CROSS_NEUROPSYCHO_WINDOW / 365) % 1) * 12))\n",
    "    \n",
    "    if sub_id not in unique_composite_subjects:\n",
    "        miss_in_removed_indices.append(sub_id)\n",
    "        continue\n",
    "    \n",
    "    # select all valid evaluations\n",
    "    sub_df = composites_neu_long.loc[[sub_id]]\n",
    "    sub_df = sub_df.loc[\n",
    "        sub_df.index.get_level_values('neurobat_date') >= init_date\n",
    "    ].copy()\n",
    "\n",
    "    # check number of follow-ups\n",
    "    if len(sub_df) < MIN_NUMBER_FOLLOWUPS:\n",
    "        miss_by_num_followups.append((sub_id, sub_date))\n",
    "        continue\n",
    "\n",
    "    # check follow-up time\n",
    "    followup_time = float(\n",
    "        np.cumsum(sub_df.index.get_level_values('neurobat_date').to_series().diff().dt.days.fillna(0).values)[-1] / 365\n",
    "    )\n",
    "\n",
    "    if followup_time < MIN_YEARS_FOLOWUP:\n",
    "        miss_by_time_followup.append((sub_id, sub_date))\n",
    "        continue\n",
    "\n",
    "    # add neuroimaging date\n",
    "    sub_df['date'] = sub_date\n",
    "    sub_df = sub_df.reset_index('neurobat_date')\n",
    "\n",
    "    # check neuropsychological window\n",
    "    if not ((sub_df['neurobat_date'] - sub_df['date']).dt.days <= CROSS_NEUROPSYCHO_WINDOW).any():\n",
    "        miss_by_neuropsycho_window.append((sub_id, sub_date))\n",
    "\n",
    "    # add the trayectory id\n",
    "    if sub_id in visited_subjects:\n",
    "        sub_df['trajectory_id'] = '{}_{}'.format(sub_id, visited_subjects[sub_id])\n",
    "        visited_subjects[sub_id] += 1\n",
    "    else:\n",
    "        sub_df['trajectory_id'] = sub_id\n",
    "        visited_subjects[sub_id] = 1\n",
    "\n",
    "    composites_neu_long_window.append(sub_df)\n",
    "\n",
    "# concatenate the trajectories\n",
    "composites_neu_long_window = pd.concat(\n",
    "    composites_neu_long_window, axis=0).reset_index().set_index(['trajectory_id', 'neurobat_date']).sort_index().copy()\n",
    "\n",
    "# recalculate follow-up time\n",
    "composites_neu_long_window['years_followup_neurobat'] = composites_neu_long_window.groupby('trajectory_id')['years_followup_neurobat'].diff().fillna(0)\n",
    "composites_neu_long_window['years_followup_neurobat'] = composites_neu_long_window.groupby('trajectory_id')['years_followup_neurobat'].cumsum()\n",
    "\n",
    "print('Number of subjects removed by matching with neuropsychological information: %d' % len(miss_in_removed_indices))\n",
    "print('Number of subjects removed by number of follow-ups: %d' % len(miss_by_num_followups))\n",
    "print('Number of subjects removed by follow-up time: %d' % len(miss_by_time_followup))\n",
    "print('Number of subjects removed by neuropsychological window: %d' % len(miss_by_neuropsycho_window))\n",
    "print('Number of unique trajectories: %d' % len(composites_neu_long_window.index.get_level_values('trajectory_id').unique()))\n",
    "print('Number of unique subjects: %d' % len(composites_neu_long_window['subject_id'].unique()))\n",
    "print('Number of entries associated with neuroimaging and neuropsychological data: %d' % len(composites_neu_long_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the remaining information from the composites used to calculate the deltas by replacing the past \n",
    "# evaluations of the subjects just selected in the previous step. This is done because for the previous \n",
    "# subjects we are only interested in tracking from the time of image acquisition.\n",
    "composites_no_trajectory = composites.loc[\n",
    "    ~composites.index.get_level_values('subject_id').isin(composites_neu_long_window['subject_id'].unique())\n",
    "].copy()\n",
    "composites_no_trajectory['trajectory_id'] = composites_no_trajectory.index.get_level_values('subject_id')\n",
    "composites_no_trajectory = composites_no_trajectory.reset_index().set_index(['trajectory_id', 'neurobat_date']).sort_index()\n",
    "composites_no_trajectory['date'] = pd.NaT\n",
    "\n",
    "composites_long = pd.concat((composites_no_trajectory, composites_neu_long_window), axis=0).sort_index()\n",
    "\n",
    "assert not composites_long.index.duplicated().any()\n",
    "\n",
    "# Generate a dataset with the remaining subjects used to pre-train the models\n",
    "\n",
    "# add information about the current follow-up\n",
    "composites_long['n_followup'] = 1\n",
    "composites_long['n_followup'] = composites_long.groupby('trajectory_id')['n_followup'].cumsum()\n",
    "\n",
    "# add information about the maximum follow-up\n",
    "composites_long = composites_long.join(\n",
    "    pd.DataFrame(composites_long.groupby('trajectory_id')['n_followup'].max())\\\n",
    "        .rename(columns={'n_followup': 'max_followup'}),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# add information about the follow-up time\n",
    "composites_long = composites_long.join(\n",
    "    pd.DataFrame(composites_long.groupby('trajectory_id')['years_followup_neurobat'].max())\\\n",
    "        .rename(columns={'years_followup_neurobat': 'max_years_followup_neurobat'}),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# select subjects with a minimum number of follow-ups and follow-up time\n",
    "followup_mask = (composites_long['max_years_followup_neurobat'] >= MIN_YEARS_FOLOWUP)\n",
    "\n",
    "print('Initial number of subjects: %d' % len(composites_long.index.get_level_values('trajectory_id').unique()))\n",
    "print('Number of subjects after filtering by follow-up time: %d' % len(composites_long.loc[followup_mask].index.get_level_values('trajectory_id').unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs an interpolation at regular time intervals on the different scores of the neuropsychological measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply an interpolation of values at regular intervals to smooth trajectories \n",
    "# (use only subjects with follow-up information)\n",
    "\n",
    "# interpolate the time series\n",
    "composites_intp = []\n",
    "for sub_id, sub_df in tqdm(composites_long.loc[followup_mask].groupby('trajectory_id'), desc='Performing interpolation...'):\n",
    "\n",
    "    # perform interpolations\n",
    "    sub_df_intp = {}\n",
    "    for var in TRAJECTORY_VARIABLES:\n",
    "        var_t, var_y = interpolateTimeSeries(\n",
    "            sub_df['years_followup_neurobat'].values, \n",
    "            sub_df[var].values, \n",
    "            step=0.5, max_t=MAX_YEARS_FOLLOWUP+0.1\n",
    "        )\n",
    "        sub_df_intp['years_followup_neurobat'] = var_t\n",
    "        sub_df_intp[var] = var_y\n",
    "        \n",
    "    # create the dataframe\n",
    "    sub_df_intp['trajectory_id'] = np.array([sub_id] * len(var_t))\n",
    "    sub_df_intp['subject_id'] = np.array([sub_df['subject_id'].values[0]] * len(var_t))\n",
    "    sub_df_intp['neurobat_date'] = np.array([sub_df.reset_index().iloc[0].loc['neurobat_date']] * len(var_t))\n",
    "    sub_df_intp = pd.DataFrame(sub_df_intp)\n",
    "    \n",
    "    # process dates\n",
    "    sub_df_intp['neurobat_date'] = pd.to_datetime(sub_df_intp['neurobat_date'])\n",
    "    sub_df_intp['neurobat_date'] = sub_df_intp.apply(lambda row: addYears(row['neurobat_date'], row['years_followup_neurobat']), axis=1)\n",
    "    sub_df_intp['neurobat_date'] = pd.to_datetime(sub_df_intp['neurobat_date'])\n",
    "    \n",
    "    # set index\n",
    "    sub_df_intp = sub_df_intp.set_index(['trajectory_id', 'neurobat_date']).sort_index()\n",
    "\n",
    "    # save results\n",
    "    composites_intp.append(sub_df_intp.copy())\n",
    "\n",
    "composites_intp = pd.concat(composites_intp, axis=0)\n",
    "composites_intp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some random trajectories\n",
    "np.random.seed(1997)\n",
    "nrows = 3\n",
    "ncols = 4\n",
    "varidx = 1\n",
    "\n",
    "sample_ids = np.random.choice(composites_intp.index.get_level_values('trajectory_id').unique(), size=nrows*ncols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(20, 8))\n",
    "for ax, _id in zip(axes.flatten(), sample_ids):\n",
    "    \n",
    "    t_intp = composites_intp.loc[_id]['years_followup_neurobat'].values\n",
    "    y_intp = composites_intp.loc[_id][TRAJECTORY_VARIABLES[varidx]].values\n",
    "    t_true = composites_long.loc[_id]['years_followup_neurobat'].values\n",
    "    y_true = composites_long.loc[_id][TRAJECTORY_VARIABLES[varidx]].values\n",
    "\n",
    "    for label, c, x, y in [\n",
    "        ('Real', 'blue', t_true, y_true),\n",
    "        ('Interp', 'orange', t_intp, y_intp),\n",
    "    ]:\n",
    "        ax.scatter(x, y, label=None, s=25, color=c)\n",
    "        ax.plot(x, y, label=label, color=c)\n",
    "    \n",
    "    ax.legend(loc='upper left')\n",
    "    setLayout(ax)\n",
    "    ax.set_title('Id: %s' % _id)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory $\\Delta$ calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A linear regression model is adjusted for each individual in order to estimate the overall tendency of the data cognitive trajectory.\n",
    "\n",
    "For subjects where the fit index of the regression model is poor ($R^2$), the slope will be calculated considering the first and last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get normative subjects\n",
    "sel_vars = ['diagnosis', 'mmse', 'cdr']\n",
    "composites_long_diag = composites_long.loc[followup_mask]\\\n",
    "    .reset_index().set_index(['subject_id', 'neurobat_date'])\\\n",
    "    .groupby(['subject_id', 'neurobat_date']).nth(0).sort_index()\\\n",
    "    .groupby('subject_id').nth([0, -1])[sel_vars].copy()\n",
    "composites_long_diag['mark'] = 1\n",
    "composites_long_diag['mark'] = composites_long_diag.groupby('subject_id')['mark'].cumsum()\n",
    "composites_long_diag = composites_long_diag\\\n",
    "    .reset_index().drop(columns=['neurobat_date']).set_index(['subject_id', 'mark'])\n",
    "composites_long_diag = composites_long_diag\\\n",
    "    .reset_index().pivot(columns=['mark'], values=sel_vars, index=['subject_id'])\n",
    "composites_long_diag.columns = ['{}_{}'.format(*c) for c in composites_long_diag.columns]\n",
    "\n",
    "normative_subjects = composites_long_diag.loc[\n",
    "    (composites_long_diag.diagnosis_1 == 'control') & \n",
    "    (composites_long_diag.diagnosis_2 == 'control') & \n",
    "    (composites_long_diag.mmse_1 >= 26) & \n",
    "    (composites_long_diag.mmse_2 >= 26) &\n",
    "    (composites_long_diag.cdr_1 == 0.0) & \n",
    "    (composites_long_diag.cdr_2 == 0.0)\n",
    "].index.tolist()\n",
    "\n",
    "\n",
    "print('Number of normative subjects: %d\\n' % len(normative_subjects))\n",
    "\n",
    "# select subjects with negative amyloid status\n",
    "amy_neg_subject_ids = composites_long.loc[composites_long.uc_berkley_negative_amyloid_1Y == 1.0]['subject_id'].unique().tolist()\n",
    "\n",
    "normative_subjects = [sub_id for sub_id in normative_subjects if sub_id in amy_neg_subject_ids]\n",
    "    \n",
    "print('(after filtering by amyloid status) Number of normative subjects: %d\\n' % len(normative_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the normative cut-off value for each composite based on the median value of the controls\n",
    "normative_cutoff_vals = composites_intp.loc[normative_subjects][TRAJECTORY_VARIABLES].quantile(0.5).to_dict()\n",
    "pprint(normative_cutoff_vals)\n",
    "\n",
    "# display normative individual trayectories compared to the rest of the population\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "axes = list(axes.flatten())\n",
    "\n",
    "for sub_id, sub_df in composites_intp.groupby('trajectory_id'):\n",
    "    for var, ax in zip(TRAJECTORY_VARIABLES, axes):\n",
    "        color = 'green' if sub_id in normative_subjects else 'grey'\n",
    "        alpha = 0.5 if sub_id in normative_subjects else 0.2\n",
    "        ax.plot(\n",
    "            sub_df['years_followup_neurobat'].values,\n",
    "            sub_df[var].values,\n",
    "            alpha=alpha, color=color\n",
    "        )\n",
    "\n",
    "for var, ax in zip(TRAJECTORY_VARIABLES, axes):\n",
    "    setLayout(ax)\n",
    "    ax.axhline(normative_cutoff_vals[var], lw=2, color='red')\n",
    "    ax.set_title(var)\n",
    "    ax.set_xlabel('Years')\n",
    "    ax.set_ylabel('Value')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "# apply the clipping\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 5))\n",
    "axes = list(axes.flatten())\n",
    "\n",
    "for ax, var in zip(axes, TRAJECTORY_VARIABLES):\n",
    "    mask = composites_intp[var] >= normative_cutoff_vals[var]\n",
    "    composites_intp.loc[mask][var].hist(alpha=0.5, density=False, label='Ignored', ax=ax)\n",
    "    composites_intp.loc[~mask][var].hist(alpha=0.5, density=False, label='No ignored', ax=ax)\n",
    "    ax.set_title(var)\n",
    "    ax.legend()\n",
    "    composites_intp.loc[mask, var] = normative_cutoff_vals[var]\n",
    "    \n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average slope per year for each subject\n",
    "r2_cutoff = 0.5\n",
    "show_plots = 10\n",
    "deltas_df = []\n",
    "bad_adjustment = []\n",
    "for sub_id, sub_df in composites_intp.groupby('trajectory_id'):\n",
    "    for var in TRAJECTORY_VARIABLES:        \n",
    "        lr = LinearRegression().fit(\n",
    "            sub_df[['years_followup_neurobat']].values,\n",
    "            sub_df[var].values\n",
    "        )\n",
    "        lr_slope = lr.coef_[0]\n",
    "\n",
    "        # calculate the goodness of fit\n",
    "        r2 = r2_score(\n",
    "            sub_df[var].values,\n",
    "            lr.predict(sub_df[['years_followup_neurobat']].values)\n",
    "        )\n",
    "        \n",
    "        # for subjects below the R2 cut-off calculate the slope considering\n",
    "        # the initial and final times\n",
    "        if r2 < r2_cutoff:\n",
    "            bad_adjustment.append(sub_id)\n",
    "            \n",
    "            # calculate the slope considering the initial and final points\n",
    "            init_val = sub_df[var].values[sub_df.years_followup_neurobat.values <= 1.0].max()\n",
    "            end_val = sub_df[var].values[sub_df.years_followup_neurobat.values > (sub_df.years_followup_neurobat.max() - 1.0)].max()\n",
    "            slope = (end_val - init_val) / sub_df.years_followup_neurobat.max()\n",
    "            \n",
    "            # recalculate the intercept baed on the new slope (optimal intercept calculated using an explicit derivation)\n",
    "            intercept = np.mean(sub_df[var].values - slope * sub_df.years_followup_neurobat.values)\n",
    "\n",
    "            # caculate the residual error\n",
    "            y_hat = (intercept + slope * sub_df.years_followup_neurobat).values\n",
    "            y_true = sub_df[var].values\n",
    "            rmse = float(np.sqrt(np.mean((y_hat - y_true)**2)))\n",
    "\n",
    "            if len(bad_adjustment) < show_plots:\n",
    "                \n",
    "                #  display the model adjustment\n",
    "                fig, ax = plt.subplots(figsize=(4, 2))\n",
    "                ax.plot(\n",
    "                    sub_df.years_followup_neurobat.values,\n",
    "                    sub_df[var].values,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    sub_df.years_followup_neurobat.values,\n",
    "                    sub_df[var].values,\n",
    "                )\n",
    "                ax.plot(\n",
    "                    [0, sub_df.years_followup_neurobat.max()],\n",
    "                    [lr.intercept_, lr.intercept_ +  lr.coef_[0] * sub_df.years_followup_neurobat.max()],\n",
    "                    lw=2, color='red', label=f'LR slope ({lr.coef_[0]:.1f})'\n",
    "                )\n",
    "                ax.plot(\n",
    "                    [0, sub_df.years_followup_neurobat.max()],\n",
    "                    [intercept, intercept +  slope * sub_df.years_followup_neurobat.max()],\n",
    "                    lw=2, color='green', label=f'Adj slope ({slope:.1f})'\n",
    "                )\n",
    "                setLayout(ax)\n",
    "                ax.legend()\n",
    "                ax.set_ylabel(var)\n",
    "                ax.set_xlabel('Years')\n",
    "                ax.set_title('{} - $r^2 = ${:.2f} (rmse = {:.2f})'.format(sub_id, r2, rmse))\n",
    "                plt.show()\n",
    "            \n",
    "            deltas_df.append({\n",
    "                'trajectory_id': sub_id,\n",
    "                'variable': var,\n",
    "                'baseline': sub_df[var].values[0],\n",
    "                'last': sub_df[var].values[-1],\n",
    "                'lr_delta': slope,\n",
    "                'lr_rmse': rmse,\n",
    "                'lr_r2': r2\n",
    "            })\n",
    "        else:\n",
    "            # caculate the residual error\n",
    "            y_hat = (float(lr.intercept_) + lr.coef_[0] * sub_df.years_followup_neurobat).values\n",
    "            y_true = sub_df[var].values\n",
    "            rmse = float(np.sqrt(np.mean((y_hat - y_true)**2)))\n",
    "\n",
    "            # save the results of the LR\n",
    "            deltas_df.append({\n",
    "                'trajectory_id': sub_id,\n",
    "                'variable': var,\n",
    "                'baseline': sub_df[var].values[0],\n",
    "                'last': sub_df[var].values[-1],\n",
    "                'lr_delta': lr.coef_[0],\n",
    "                'lr_rmse': rmse,\n",
    "                'lr_r2': r2\n",
    "            })\n",
    "\n",
    "# create the dataframe\n",
    "deltas_df = pd.DataFrame(deltas_df)\n",
    "\n",
    "# pivot the dataframe\n",
    "deltas_df = deltas_df.pivot(\n",
    "    index=['trajectory_id'],\n",
    "    columns=['variable'],\n",
    "    values=['baseline', 'last', 'lr_delta', 'lr_rmse', 'lr_r2']\n",
    ")\n",
    "\n",
    "# flat columns\n",
    "deltas_df.columns = ['{}_{}'.format(e[0], e[1]) for e in deltas_df.columns]\n",
    "deltas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display R2\n",
    "deltas_df[[c for c in deltas_df.columns if 'r2' in c]].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut-off definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this part of the code, cut-off points of normality/non-normality are established according to the distribution of deltas in the group of normative control subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the stable vs decline trajectory\n",
    "cognitive_normal_cutoff = 75           # required starting/end value of the cognitive normal subject\n",
    "cognitive_normal_quantile = 0.05        # samples below the X% of the CN subjects will be marked as decliners\n",
    "delta_cutoffs = {}\n",
    "\n",
    "for var in TRAJECTORY_VARIABLES:\n",
    "    cutoff = cognitive_normal_cutoff\n",
    "    quantile = cognitive_normal_quantile\n",
    "\n",
    "    # calculate the mask of normative subjects\n",
    "    var_mask = (\n",
    "        (deltas_df.loc[normative_subjects]['baseline_%s' % var] >= cutoff) & \n",
    "        (deltas_df.loc[normative_subjects]['last_%s' % var] >= cutoff))\n",
    "    delta_var = 'lr_delta_%s' % var\n",
    "\n",
    "    print('(%s) Number of normative subjects: %d' % (var, var_mask.sum()))\n",
    "    delta_cutoffs[delta_var] = deltas_df.loc[normative_subjects].loc[var_mask][delta_var].quantile(quantile)\n",
    "    print('Cut-off: %.3f' % delta_cutoffs[delta_var])\n",
    "\n",
    "    # apply the cut-off\n",
    "    delta_var_bin = (deltas_df[delta_var] < delta_cutoffs[delta_var])\n",
    "    deltas_df['%s_binary' % delta_var] = delta_var_bin.astype(int)\n",
    "\n",
    "    print('Percentage of positive class: {:.2f}%'.format(delta_var_bin.mean() * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anlayze the calcualted deltas (code adapted from other versions... a little bit messy)\n",
    "composites_long_with_deltas = composites_long.join(deltas_df)\n",
    "composites_long_with_deltas_baseline = composites_long_with_deltas.groupby('trajectory_id').nth(0)\n",
    "composites_long_with_deltas_last = composites_long_with_deltas.groupby('trajectory_id').nth(-1)\n",
    "composites_long_with_deltas_baseline = \\\n",
    "    composites_long_with_deltas_baseline.reset_index('neurobat_date').rename(columns={'diagnosis': 'baseline_diagnosis'}).join(\n",
    "        composites_long_with_deltas_last.reset_index('neurobat_date')[['diagnosis']].rename(columns={'diagnosis': 'last_diagnosis'})\n",
    ")\n",
    "composites_long_with_deltas_baseline['diagnosis'] = composites_long_with_deltas_baseline['baseline_diagnosis']\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for var in TRAJECTORY_VARIABLES:\n",
    "        delta_var = 'lr_delta_%s' % var\n",
    "\n",
    "        # plot distributions in baseline and last diagnosis\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        axes = [ax for ax in axes.flatten()]\n",
    "        \n",
    "        for ax, flag in zip(axes, ['baseline', 'last']):\n",
    "            sns.boxplot(\n",
    "                data=composites_long_with_deltas_baseline,\n",
    "                y=delta_var,\n",
    "                x='diagnosis',\n",
    "                notch=True, showcaps=False,\n",
    "                flierprops={\"marker\": \"x\"},\n",
    "                boxprops={\"facecolor\": (.3, .5, .7, .5)},\n",
    "                medianprops={\"color\": \"r\", \"linewidth\": 2},\n",
    "                zorder=2,\n",
    "                ax=ax\n",
    "            )\n",
    "            sns.stripplot(\n",
    "                data=composites_long_with_deltas_last,\n",
    "                x='diagnosis', \n",
    "                y=delta_var,  \n",
    "                color=\"0.3\", \n",
    "                alpha=0.3,\n",
    "                zorder=1,\n",
    "                ax=ax,\n",
    "            \n",
    "            )\n",
    "            ax.axhline(delta_cutoffs['lr_delta_%s' % var], lw=4, color='#00FF00')\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_title('Diagnosis (%s)' % flag, size=15)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.grid(alpha=0.3, color='black')\n",
    "        fig.suptitle(var, y=1.05, size=17)\n",
    "        plt.show()\n",
    "        \n",
    "        display(pd.concat([\n",
    "            pd.DataFrame(\n",
    "                composites_long_with_deltas_baseline.groupby(['baseline_diagnosis', 'last_diagnosis'])[delta_var].count()\n",
    "            ).rename(columns={'lr_delta_%s' % var: 'count_%s' % var}),\n",
    "            pd.DataFrame(\n",
    "                composites_long_with_deltas_baseline.groupby(['baseline_diagnosis', 'last_diagnosis'])[delta_var].mean()\n",
    "            ).rename(columns={'lr_delta_%s' % var: 'mean_delta_%s' % var}).round(decimals=2),\n",
    "            pd.DataFrame(\n",
    "                composites_long_with_deltas_baseline.groupby(['baseline_diagnosis', 'last_diagnosis'])[delta_var].quantile(0.10)\n",
    "            ).rename(columns={'lr_delta_%s' % var: 'q10_delta_%s' % var}).round(decimals=2),\n",
    "            pd.DataFrame(\n",
    "                composites_long_with_deltas_baseline.groupby(['baseline_diagnosis', 'last_diagnosis'])[delta_var].quantile(0.25)\n",
    "            ).rename(columns={'lr_delta_%s' % var: 'q25_delta_%s' % var}).round(decimals=2),\n",
    "            pd.DataFrame(\n",
    "                composites_long_with_deltas_baseline.groupby(['baseline_diagnosis', 'last_diagnosis'])[delta_var].quantile(0.90)\n",
    "            ).rename(columns={'lr_delta_%s' % var: 'q90_delta_%s' % var}).round(decimals=2)\n",
    "        ], axis=1))\n",
    "        \n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final database exportation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the deltas dataframe for those subjects with associated neuroimaging data and individuals with a baseline diagnosis different from dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables of interest for the rest of clinical information\n",
    "composites_long_with_deltas = composites_long.loc[deltas_df.index].copy()\n",
    "composites_long_with_deltas = composites_long_with_deltas[[\n",
    "    'subject_id', 'diagnosis', 'yschooling', 'sex_1M_2F', \n",
    "    'age', 'mmse', 'cdr', 'years_followup_neurobat',\n",
    "    'date',\n",
    "\n",
    "    # add neuropsychological test\n",
    "    'memory_avlt_trial_1',\n",
    "    'memory_avlt_trial_2',\n",
    "    'memory_avlt_trial_6',\n",
    "    'memory_avlt_delayed',\n",
    "    'memory_avlt_recognition',\n",
    "    'memory_word_recognition',\n",
    "    'memory_word_recall_delayed',\n",
    "    'memory_word_recall',\n",
    "    'language_cat_fluency',\n",
    "    'language_naming', \n",
    "    'language_bnt_tot',\n",
    "    'language_word_finding_diff',\n",
    "    'exec_tmt_a_time',\n",
    "    'exec_tmt_b_time',\n",
    "    'attention_number_cancellation',\n",
    "    'visuos_clock_copy_tot_score',\n",
    "    'visuos_clock_draw_tot_score',\n",
    "    'visuos_constructional_praxis',\n",
    "    'visuos_ideational_praxis'\n",
    "]].sort_index().copy()\n",
    "\n",
    "# pivot the baseline and last information\n",
    "composites_long_with_deltas_baseline = composites_long_with_deltas.reset_index('neurobat_date').groupby('trajectory_id').nth(0)\n",
    "composites_long_with_deltas_baseline.columns = ['baseline_%s' % c for c in composites_long_with_deltas_baseline.columns]\n",
    "composites_long_with_deltas_last = composites_long_with_deltas.reset_index('neurobat_date').groupby('trajectory_id').nth(-1)\n",
    "composites_long_with_deltas_last.columns = ['last_%s' % c for c in composites_long_with_deltas_last.columns]\n",
    "composites_long_with_deltas = pd.concat([\n",
    "    composites_long_with_deltas_baseline, composites_long_with_deltas_last\n",
    "], axis=1)\n",
    "composites_long_with_deltas = composites_long_with_deltas\\\n",
    "    .drop(columns=['last_subject_id', 'last_date'])\\\n",
    "    .rename(columns={'baseline_subject_id': 'subject_id', 'baseline_date': 'date'})\n",
    "\n",
    "# add clinical information to the calculated deltas (add basline and last evaluation to the deltas)\n",
    "deltas_df = deltas_df.join(composites_long_with_deltas, how='inner')\n",
    "\n",
    "# add neuroimaging infomration to the calculated deltas\n",
    "deltas_with_neuroimaging_df =\\\n",
    "    deltas_df.dropna(subset=['date']).reset_index().set_index(['subject_id', 'date']).sort_index().join(\n",
    "        mri_fdg_amy_baseline, how='inner'\n",
    ")\n",
    "\n",
    "# remove baseline diagnosis of dementia\n",
    "deltas_with_neuroimaging_df = deltas_with_neuroimaging_df.loc[deltas_with_neuroimaging_df.baseline_diagnosis != 'dementia'].copy()\n",
    "\n",
    "# transform trajectory variables to float 32\n",
    "qual_delta_vars = [c for c in deltas_with_neuroimaging_df.columns if c.startswith('lr_delta')]\n",
    "deltas_with_neuroimaging_df[qual_delta_vars] = deltas_with_neuroimaging_df[qual_delta_vars].astype(np.float32)\n",
    "\n",
    "# add information of 2Y and 4Y diagnosis\n",
    "deltas_with_neuroimaging_df = deltas_with_neuroimaging_df\\\n",
    "    .reset_index()\\\n",
    "    .set_index(['trajectory_id', 'baseline_neurobat_date'])\\\n",
    "    .join(\n",
    "        composites_long[['diagnosis_2Y', 'diagnosis_4Y']]\\\n",
    "            .fillna(np.nan).reset_index()\\\n",
    "            .rename(columns={'neurobat_date': 'baseline_neurobat_date'})\\\n",
    "            .set_index(['trajectory_id', 'baseline_neurobat_date']),\n",
    "        how='left')\\\n",
    "    .reset_index()\\\n",
    "    .set_index(['subject_id', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some sample statistics\n",
    "sample_demo = []\n",
    "for diag, diag_df in deltas_with_neuroimaging_df.groupby('baseline_diagnosis'):\n",
    "    diag_des_dat = {\n",
    "        'Diagnosis': diag, \n",
    "        'Sample size (%)': '{:.0f} ({:.1f})'.format(diag_df.shape[0], diag_df.shape[0] / deltas_with_neuroimaging_df.shape[0] * 100),\n",
    "        'Age (Mean, SD)': '{:.1f} ({:.1f})'.format(\n",
    "            diag_df['baseline_age'].mean(), \n",
    "            diag_df['baseline_age'].std()),\n",
    "        'Sex (% Female)': '{:.1f}'.format(\n",
    "             (diag_df['baseline_sex_1M_2F'] -1).mean() * 100\n",
    "        ),\n",
    "        'Years of formal education (Mean, SD)': '{:.1f} ({:.1f})'.format(\n",
    "            diag_df['baseline_yschooling'].mean(), \n",
    "            diag_df['baseline_yschooling'].std()),\n",
    "        'MMSE (Mean, SD)': '{:.1f} ({:.1f})'.format(\n",
    "            diag_df['baseline_mmse'].mean(), \n",
    "            diag_df['baseline_mmse'].std()),\n",
    "        'Years followup (Mean, SD)': '{:.1f} ({:.1f})'.format(\n",
    "            diag_df['last_years_followup_neurobat'].mean(), \n",
    "            diag_df['last_years_followup_neurobat'].std()),\n",
    "    }\n",
    "        \n",
    "    sample_demo.append(\n",
    "        diag_des_dat\n",
    "    )\n",
    "sample_demo = pd.DataFrame(sample_demo).set_index('Diagnosis').loc[[\n",
    "    'control', \n",
    "    'mci'\n",
    "]]\n",
    "sample_demo.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some delta-related statistics\n",
    "#   - One row per diagnosis transition\n",
    "#   - Columns: Average delta in each cognitive domain and percentage of decliners per each cognitive domain\n",
    "temp_df = deltas_with_neuroimaging_df.copy()\n",
    "\n",
    "# calculate the number of affected cognitive domains\n",
    "temp_df['num_affected_cd'] = temp_df[[c for c in temp_df.columns if 'binary' in c]].sum(axis=1)\n",
    "\n",
    "# calculate diagnosis transitions\n",
    "deltas_info = {}\n",
    "temp_df['diag_transition'] = temp_df['baseline_diagnosis'] + '-' + temp_df['last_diagnosis']\n",
    "temp_df.loc[temp_df['diag_transition'].isin(['control-control']), 'diag_transition'] = 'sCN'\n",
    "temp_df.loc[temp_df['diag_transition'].isin(['control-mci', 'control-dementia']), 'diag_transition'] = 'pCN'\n",
    "temp_df.loc[temp_df['diag_transition'].isin(['mci-mci']), 'diag_transition'] = 'sMCI'\n",
    "temp_df.loc[temp_df['diag_transition'].isin(['mci-dementia']), 'diag_transition'] = 'pMCI'\n",
    "temp_df.loc[temp_df['diag_transition'].isin(['mci-control']), 'diag_transition'] = 'rMCI'\n",
    "for diag_trans in ['sCN', 'pCN', 'sMCI', 'pMCI', 'rMCI']:\n",
    "    diag_trans_df = temp_df.loc[temp_df['diag_transition'] == diag_trans]\n",
    "    domain_info = {}\n",
    "    for fname, dname in [\n",
    "        ('Executive', 'lr_delta_exec_composite'),\n",
    "        ('Memory', 'lr_delta_memory_composite'),\n",
    "        ('Language', 'lr_delta_language_composite'),\n",
    "        ('Visuospatial', 'lr_delta_visuospatial_composite')]:\n",
    "        domain_info[f'{fname} - quant'] = f'{diag_trans_df[dname].mean():.1f} ({diag_trans_df[dname].std():.1f})'\n",
    "        domain_info[f'{fname} - decliners'] = \\\n",
    "            f'{diag_trans_df[f\"{dname}_binary\"].sum():.0f} ({(diag_trans_df[f\"{dname}_binary\"].mean())*100:.1f})'\n",
    "            #f'{diag_trans_df[f\"{dname}_binary\"].sum():.0f} ({(diag_trans_df[f\"{dname}_binary\"].sum() / temp_df.shape[0])*100:.1f})'\n",
    "\n",
    "    # calculate the individuals with more than 1 domain affected\n",
    "    more_than_2_domains = (diag_trans_df[[c for c in diag_trans_df.columns if c.endswith('binary')]].sum(axis=1) > 1)\n",
    "    domain_info['> 2 affected domains'] = f'{float(more_than_2_domains.sum()):.0f} ({float(more_than_2_domains.mean()) * 100:.1f})'\n",
    "\n",
    "    deltas_info[f'{diag_trans} ({diag_trans_df.shape[0]})'] = domain_info\n",
    "\n",
    "deltas_info = pd.DataFrame(deltas_info).T\n",
    "deltas_info = deltas_info[[c for c in deltas_info.columns if 'quant' in c] + [c for c in deltas_info.columns if 'decliners' in c] + ['> 2 affected domains']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some delta-related statistics (2)\n",
    "sample_traj = []\n",
    "for fname, dname in [\n",
    "    ('Executive', 'lr_delta_exec_composite'),\n",
    "    ('Memory', 'lr_delta_memory_composite'),\n",
    "    ('Language', 'lr_delta_language_composite'),\n",
    "    ('Visuospatial', 'lr_delta_visuospatial_composite')]:\n",
    "    domain_info = {}\n",
    "    for group, group_df in deltas_with_neuroimaging_df.groupby('%s_binary' % dname):\n",
    "        domain_info[group] = {}\n",
    "\n",
    "        # add delta statistics\n",
    "        domain_info[group]['Delta (Mean, SD)'] =\\\n",
    "            '{:.2f} ({:.2f})'.format(group_df[dname].mean(), group_df[dname].std())\n",
    "        group_diag_trans = (group_df['baseline_diagnosis'] + '-' + group_df['last_diagnosis']).value_counts().to_dict()\n",
    "        \"\"\"\n",
    "        domain_info[group]['Stable control'] = f\"{group_diag_trans.get('control-control', 0.0) / group_df.shape[0] * 100:.1f}\"\n",
    "        domain_info[group]['Stable MCI'] = f\"{group_diag_trans.get('mci-mci', 0.0) / group_df.shape[0] * 100:.1f}\"\n",
    "        domain_info[group]['MCI converter'] = f\"{group_diag_trans.get('control-mci', 0.0) / group_df.shape[0] * 100:.1f}\"\n",
    "        domain_info[group]['Demencia converter'] = f\"{(group_diag_trans.get('mci-dementia', 0.0) + group_diag_trans.get('control-dementia', 0.0) ) / group_df.shape[0] * 100:.1f}\"\n",
    "        \"\"\"\n",
    "        stable_diagnosis = (\n",
    "            group_diag_trans.get('control-control', 0.0) + \n",
    "            group_diag_trans.get('mci-mci', 0.0) + \n",
    "            group_diag_trans.get('mci-control', 0.0) \n",
    "        )\n",
    "        progresive_diagnosis = (\n",
    "            group_diag_trans.get('control-dementia', 0.0) + \n",
    "            group_diag_trans.get('control-mci', 0.0) + \n",
    "            group_diag_trans.get('mci-dementia', 0.0) \n",
    "        )\n",
    "        domain_info[group]['Stable diagnosis'] = f\"{stable_diagnosis / group_df.shape[0] * 100:.1f}\"\n",
    "        domain_info[group]['Progressive diagnosis'] = f\"{progresive_diagnosis / group_df.shape[0] * 100:.1f}\"\n",
    "\n",
    "\n",
    "    # save the dataframe\n",
    "    domain_info_df = pd.DataFrame(domain_info).T\n",
    "    domain_info_df['Domain'] = fname\n",
    "    domain_info_df.index.names = ['Group']\n",
    "    domain_info_df = domain_info_df.reset_index().set_index(['Domain', 'Group'])\n",
    "    sample_traj.append(domain_info_df)\n",
    "\n",
    "sample_traj = pd.concat(sample_traj, axis=0).sort_index()\n",
    "sample_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the generated dataframe\n",
    "deltas_with_neuroimaging_df.to_parquet(\n",
    "    BASE_STUDY_PATH / 'mounts' / 'v1' / ('%s_longitudinal.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-sectional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of cross-sectional data for pre-training of models using the different neuroimaging modalities independently. At this point, all images that have been included in the longitudinal data set have been excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the neuropsychological data\n",
    "composites_cross_seccional = composites.fillna(np.nan).copy()\n",
    "\n",
    "# add delta information to the neuropsychological data\n",
    "composites_cross_seccional = composites_cross_seccional.join(\n",
    "    deltas_df\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={'baseline_neurobat_date': 'neurobat_date'})\\\n",
    "        .set_index(['subject_id', 'neurobat_date'])[[\n",
    "        'lr_delta_exec_composite', 'lr_delta_language_composite', \n",
    "        'lr_delta_memory_composite', 'lr_delta_visuospatial_composite',\n",
    "        'lr_delta_exec_composite_binary', 'lr_delta_language_composite_binary', \n",
    "        'lr_delta_memory_composite_binary', 'lr_delta_visuospatial_composite_binary'\n",
    "    ]],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target modling variables for model pretraining\n",
    "target_vars = [\n",
    "    'diagnosis',\n",
    "    'diagnosis_2Y',\n",
    "    'diagnosis_4Y',\n",
    "    'memory_composite', \n",
    "    'exec_composite',\n",
    "    'language_composite', \n",
    "    'visuospatial_composite',\n",
    "    'lr_delta_exec_composite', \n",
    "    'lr_delta_language_composite', \n",
    "    'lr_delta_memory_composite', \n",
    "    'lr_delta_visuospatial_composite'\n",
    "]\n",
    "\n",
    "# add the neuropsychological data to the neuroimaging data\n",
    "cross_sectional_datasets = {}\n",
    "for mod_key, mod_df in cross_mod_data.items():\n",
    "    init_entries = mod_df.shape[0]\n",
    "    init_subjects = len(mod_df.index.get_level_values('subject_id').unique())\n",
    "\n",
    "    # cross the data\n",
    "    crossed_mod_df = crossNearestInfo(\n",
    "        target_df=mod_df,\n",
    "        source_df=composites_cross_seccional,\n",
    "        how='left',\n",
    "        window=CROSS_NEUROPSYCHO_WINDOW\n",
    "    )#.drop_duplicates()\n",
    "\n",
    "    # remove duplicated entries in the target variables\n",
    "    crossed_mod_df = crossed_mod_df.loc[\n",
    "        ~crossed_mod_df[target_vars].duplicated()\n",
    "    ].copy()\n",
    "\n",
    "    # for MRI filter subjects without delta values or 2Y/4Y diagnosis\n",
    "    if mod_key == 'mri':\n",
    "        missing_mask = (\n",
    "            (\n",
    "                crossed_mod_df[[\n",
    "                    'lr_delta_exec_composite', 'lr_delta_language_composite', \n",
    "                    'lr_delta_memory_composite',  'lr_delta_visuospatial_composite'\n",
    "            ]].isna().sum(axis=1) == 4) & (\n",
    "                crossed_mod_df[[\n",
    "                    'diagnosis_2Y',\n",
    "                    'diagnosis_4Y'\n",
    "            ]].isna().sum(axis=1) == 2)\n",
    "        )\n",
    "        crossed_mod_df = crossed_mod_df.loc[~missing_mask].copy()\n",
    "\n",
    "\n",
    "    end_entries = crossed_mod_df.shape[0]\n",
    "    end_subjects = len(crossed_mod_df.index.get_level_values('subject_id').unique())\n",
    "\n",
    "    # save the generated data\n",
    "    cross_sectional_datasets[mod_key] = crossed_mod_df\n",
    "\n",
    "    print('(%s) Number of subjects: %d -> %d' % (\n",
    "        mod_key, init_subjects, end_subjects\n",
    "    ))\n",
    "    print('(%s) Number of entries: %d -> %d' % (\n",
    "        mod_key, init_entries, end_entries\n",
    "    ))\n",
    "\n",
    "    display(pd.DataFrame(\n",
    "        crossed_mod_df[target_vars].isna().sum() / crossed_mod_df.shape[0] * 100,\n",
    "        columns=['perc_missing_values']\n",
    "    ).round(2))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the generated datasets\n",
    "for mod_key, mod_df in cross_sectional_datasets.items():\n",
    "    mod_df.to_parquet(\n",
    "        os.path.join(\n",
    "            BASE_STUDY_PATH, 'mounts', 'v1', '%s_%s_cross_sectional.parquet' % (\n",
    "                datetime.now().strftime('%Y%m%d'), mod_key\n",
    "        ))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AD-GNN-v4",
   "language": "python",
   "name": "ad-gnn-v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
