{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ee4295",
   "metadata": {},
   "source": [
    "Notebook used to create the clinical dataset from ADNI neuropsychological assessments and clinical evaluations.\n",
    "\n",
    "> **IMPORTANT**: The data used as input for this notebook must be downloaded directly from the ADNI website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path definition\n",
    "PATH_TO_ADNI_DATA = Path(os.path.join('..', 'data', 'adni'))\n",
    "PATH_TO_OUTPUT_DATA = PATH_TO_ADNI_DATA / 'processed'\n",
    "PATH_TO_DOWNLOAD_DATA = PATH_TO_ADNI_DATA / 'download'\n",
    "\n",
    "# diagnostic information files\n",
    "PATH_TO_DXSUM = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_DXSUM_28Apr2024.csv'\n",
    "PATH_TO_DEMO = PATH_TO_DOWNLOAD_DATA / 'PTDEMOG_27Mar2025.csv'\n",
    "\n",
    "# neuropsychological information files\n",
    "PATH_TO_NEUROBAT = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_NEUROBAT_28Apr2024.csv'\n",
    "PATH_TO_ADAS_1 = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_ADASSCORES_28Apr2024.csv'\n",
    "PATH_TO_ADAS_GO23 = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_ADAS_ADNIGO23_28Apr2024.csv'\n",
    "\n",
    "# MMSE and CDR information\n",
    "PATH_TO_MMSE = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_MMSE_28Apr2024.csv'\n",
    "PATH_TO_CDR = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_CDR_28Apr2024.csv'\n",
    "\n",
    "# Neuroimaging QC information files\n",
    "PATH_TO_AMY_QC1 = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_AMYQC_28Apr2024.csv'\n",
    "PATH_TO_AMY_QC2 = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_AV45QC_28Apr2024.csv'\n",
    "PATH_TO_FDG_QC1 = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_PETC3_28Apr2024.csv'\n",
    "PATH_TO_FDG_QC2 = PATH_TO_DOWNLOAD_DATA / 'All_Subjects_PETQC_28Apr2024.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a46cb",
   "metadata": {},
   "source": [
    "# Diagnostic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapDiagnostic(v):\n",
    "    \"\"\" Function used to map the etiologies \"\"\"\n",
    "    if pd.isna(v) or v == '-4':\n",
    "        return np.nan\n",
    "\n",
    "    mapping = {\n",
    "        '1': 'ftd', '2': 'parkinson', '3': 'huntington', '4': 'psp',\n",
    "        '5': 'oh', '6': 'nph', '7': 'mdd', '8': 'corticobasal',\n",
    "        '9': 'vascular', '10': 'prion', '11': 'hiv', '12': 'ppa',\n",
    "        '13': 'corticopost', '14': 'other'\n",
    "    }\n",
    "\n",
    "    if isinstance(v, (float, int)):\n",
    "        v = str(int(v))  # Convertir nÃºmeros a strings sin decimales\n",
    "    \n",
    "    if isinstance(v, str):\n",
    "        return '|'.join(filter(None, (mapping.get(diag) for diag in v.split('|'))))\n",
    "\n",
    "    raise ValueError(f\"Invalid input type: {type(v)}\")\n",
    "\n",
    "\n",
    "def calculateFutureVarValue(\n",
    "    df: pd.DataFrame,\n",
    "    y_followup: int,\n",
    "    m_window: int,\n",
    "    target_var: str,\n",
    "    group_id: str,\n",
    "    date_id: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Subroutine used to calculate the value of a given variable X years in the future\n",
    "    timmed by a X window constraint (in months) \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['years_diff_%dY' % y_followup] = np.nan\n",
    "    df['%s_%dY' % (target_var, y_followup)] = np.nan\n",
    "    df['date_%dY' %  y_followup] = np.nan\n",
    "    df['__date'] = df.index.get_level_values(date_id)\n",
    "    new_df = []\n",
    "    for sub_id, sub_df in tqdm(df.groupby(group_id), desc='Crossing data...'):\n",
    "        \n",
    "        # subject without followup\n",
    "        if sub_df.shape[0] == 1:\n",
    "            new_df.append(sub_df)\n",
    "            continue\n",
    "    \n",
    "        # create all possible row combinations\n",
    "        sub_df_cross = sub_df.reset_index()[[group_id, date_id, target_var, '__date']].merge(sub_df.reset_index()[[target_var, '__date']], how='cross')\n",
    "        \n",
    "        # select only entries in window\n",
    "        sub_df_cross = sub_df_cross.loc[\n",
    "            ((sub_df_cross['__date_y'] - sub_df_cross['__date_x']).dt.days - 365 * y_followup).abs() < (m_window*30)]\n",
    "    \n",
    "        # select/create variables of interest\n",
    "        sub_df_cross['years_diff_%dY' % y_followup] = (sub_df_cross['__date_y'] - sub_df_cross['__date_x']).dt.days / 365\n",
    "        sub_df_cross['%s_%dY' % (target_var, y_followup)] = sub_df_cross['%s_y' % target_var]\n",
    "        sub_df_cross['date_%dY' %  y_followup] = sub_df_cross['__date_y']\n",
    "    \n",
    "        # add information\n",
    "        sub_df_cross = sub_df_cross.set_index([group_id, date_id])[[\n",
    "            'years_diff_%dY' % y_followup,\n",
    "            '%s_%dY' % (target_var, y_followup),\n",
    "            'date_%dY' %  y_followup\n",
    "        ]]\n",
    "        \n",
    "        if sub_df_cross.index.duplicated().any():\n",
    "            # select the entry closests to the target followup period\n",
    "            sub_df_cross['__grouping_ind'] = (sub_df_cross['years_diff_%dY' % y_followup] - y_followup).abs()\n",
    "            sub_df_cross = sub_df_cross\\\n",
    "                .reset_index().set_index([group_id, date_id, '__grouping_ind']).sort_index()\\\n",
    "                .groupby([group_id, date_id]).nth(0).reset_index('__grouping_ind').drop(columns=['__grouping_ind'])\n",
    "            assert not sub_df_cross.index.duplicated().any()\n",
    "        else:\n",
    "            sub_df.loc[sub_df_cross.index, sub_df_cross.columns] = sub_df_cross\n",
    "    \n",
    "        new_df.append(sub_df.copy())\n",
    "        \n",
    "    # format the final dataframe\n",
    "    new_df = pd.concat(new_df, axis=0)\n",
    "    new_df['date_%dY' % y_followup] = pd.to_datetime(new_df['date_%dY' % y_followup])\n",
    "    new_df = new_df.drop(columns=['__date'])\n",
    "\n",
    "    assert new_df.shape[0] == df.shape[0], 'Shape missmatch'\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60888c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load diagnostic information\n",
    "dx = pd.read_csv(PATH_TO_DXSUM)\n",
    "print('Initial shape: %d' % dx.shape[0])\n",
    "\n",
    "# remove entries without date\n",
    "dx = dx.dropna(subset=['EXAMDATE', 'PTID'])\n",
    "print('Shape after removing key variables: %d' % dx.shape[0])\n",
    "\n",
    "# process variables\n",
    "dx['EXAMDATE'] = pd.to_datetime(dx['EXAMDATE'])\n",
    "dx = dx.rename(columns={\n",
    "    'PTID': 'subject_id',\n",
    "    'EXAMDATE': 'diag_date'\n",
    "})\n",
    "\n",
    "# Codify variables. Variable codes:\n",
    "#\n",
    "#     - DIAGNOSIS: 1=CN, 2=MCI, 3=Dementia\n",
    "dx['DIAGNOSIS'] = dx['DIAGNOSIS'].apply(\n",
    "    lambda v: {1: 'control', 2: 'mci', 3: 'dementia'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DIAGNOSIS': 'diagnosis'})\n",
    "\n",
    "#     - DXMPTR3: (Petersen) Normal general cognitive function (1=Yes; 0=No; 2=Marginal)\n",
    "dx['DXMPTR3'] = dx['DXMPTR3'].apply(\n",
    "    lambda v: {1: 'yes', 0: 'no', 2: 'marginal'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXMPTR3': 'petersen_normal_cognitive_func'})\n",
    "\n",
    "#     - DXMPTR4: (Petersen) Normal activities of daily living (1=Yes; 0=No; 2=Marginal)\n",
    "dx['DXMPTR4'] = dx['DXMPTR4'].apply(\n",
    "    lambda v: {1: 'yes', 0: 'no', 2: 'marginal'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXMPTR4': 'petersen_normal_daily_living'})\n",
    "\n",
    "#     - DXMPTR5: (Petersen) Objective memory impairment for age and education (1=Yes; 0=No)\n",
    "dx['DXMPTR5'] = dx['DXMPTR5'].apply(\n",
    "    lambda v: {1: 'yes', 0: 'no'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXMPTR5': 'petersen_objective_ci'})\n",
    "\n",
    "#     - DXMPTR6: (Petersen) Not demented by diagnostic criteria (1=Yes; 0=No)\n",
    "# IMPORTANT: I have reversed the logic\n",
    "dx['DXMPTR6'] = dx['DXMPTR6'].apply(\n",
    "    lambda v: {0: 'yes', 1: 'no'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXMPTR6': 'petersen_demented_by_diag'})\n",
    "\n",
    "#     - DXMDUE: Suspected cause of MCI (1=MCI due to Alzheimer's Disease; 2=MCI due to other etiology)\n",
    "dx['DXMDUE'] = dx['DXMDUE'].apply(\n",
    "    lambda v: {1: 'ad', 2: 'other'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXMDUE': 'cause_mci'})\n",
    "\n",
    "#     - DXMOTHET: If MCI due to other etiology, select box(es) to indicate reason (1=Fronto-temporal \n",
    "#                 Dementia; 2=Parkinson's Disease; 3=Huntington's Disease; 4=Progressive Supranuclear \n",
    "#                 Palsy; 5=Alcohol-related Dementia; 6=NPH; 7=Major Depression; 8=Corticobasal Degeneration; \n",
    "#                 9=Vascular Dementia; 10=Prion-Associated Dementia; 11=HIV; 12=Primary Progressive Aphasia; \n",
    "#                 13=Posterior Cortical Dysfunction; 14=Other (specify))\n",
    "dx['DXMOTHET'] = dx['DXMOTHET'].apply(mapDiagnostic)\n",
    "dx = dx.rename(columns={'DXMOTHET': 'cause_mci_non_ad_eth'})\n",
    "\n",
    "# merge this variable with cause_mci\n",
    "dx.loc[dx.cause_mci != 'ad', 'cause_mci'] =\\\n",
    "    dx.loc[dx.cause_mci != 'ad', 'cause_mci_non_ad_eth']\n",
    "\n",
    "#     - DXDSEV: Dementia Severity - Clinician's Impression (1=Mild; 2=Moderate; 3=Severe)\n",
    "dx['DXDSEV'] = dx['DXDSEV'].apply(\n",
    "    lambda v: {1: 'mild', 2: 'moderate', 3: 'severe'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXDSEV': 'dementia_stage'})\n",
    "\n",
    "#     - DXAPP: If Dementia due to Alzheimer's Disease, indicate likelihood (1=Probable; 2=Possible)\n",
    "dx['DXAPP'] = dx['DXAPP'].apply(\n",
    "    lambda v: {1: 'ad_probable', 2: 'ad_possible'}.get(int(v), np.nan) if not pd.isna(v) else np.nan\n",
    ")\n",
    "dx = dx.rename(columns={'DXAPP': 'cause_dementia'})\n",
    "\n",
    "#     - DXODES: If dementia due to other etiology, select best diagnosis: 1=Fronto-temporal Dementia; \n",
    "#               2=Parkinson's Disease; 3=Huntington's Disease; 4=Progressive Supranuclear Palsy; 5=Alcohol-related \n",
    "#               Dementia; 6=NPH; 7=Major Depression; 8=Corticobasal Degeneration; 9=Vascular Dementia; 10=Prion-\n",
    "#               Associated Dementia; 11=HIV; 12=Primary Progressive Aphasia; 13=Posterior Cortical Dysfunction; 14=Other (specify)\n",
    "dx['DXODES'] = dx['DXODES'].apply(mapDiagnostic)\n",
    "dx = dx.rename(columns={'DXODES': 'cause_dementia_non_ad_eth'})\n",
    "\n",
    "# merge this variable with cause_dementia\n",
    "dx.loc[~dx.cause_dementia.isin(['ad_probable', 'ad_possible']), 'cause_dementia'] =\\\n",
    "    dx.loc[~dx.cause_dementia.isin(['ad_probable', 'ad_possible']), 'cause_mci_non_ad_eth']\n",
    "\n",
    "# create a primary diagnosis\n",
    "dx['primary_diagnosis'] = np.nan\n",
    "dx.loc[(dx['diagnosis'] == 'dementia'), 'primary_diagnosis'] =\\\n",
    "    dx.loc[(dx['diagnosis'] == 'dementia'), 'cause_dementia']\n",
    "\n",
    "dx.loc[(dx['diagnosis'] == 'mci'), 'primary_diagnosis'] =\\\n",
    "    dx.loc[(dx['diagnosis'] == 'mci'), 'cause_dementia']\n",
    "dx.loc[(dx['diagnosis'] == 'mci'), 'primary_diagnosis'] =\\\n",
    "    dx.loc[(dx['diagnosis'] == 'mci'), 'cause_mci']\n",
    "\n",
    "dx.loc[(dx['diagnosis'] == 'control'), 'primary_diagnosis'] =\\\n",
    "    dx.loc[(dx['diagnosis'] == 'control'), 'cause_dementia']\n",
    "dx.loc[(dx['diagnosis'] == 'control'), 'primary_diagnosis'] =\\\n",
    "    dx.loc[(dx['diagnosis'] == 'control'), 'cause_mci']\n",
    "\n",
    "dx.loc[(dx['diagnosis'] == 'control') & (dx['primary_diagnosis'].isna()), 'primary_diagnosis'] = 'control'\n",
    "\n",
    "# format the index and select the variables of interest removing duplicated\n",
    "dx = dx[[\n",
    "    'subject_id',\n",
    "    'diag_date',\n",
    "    'diagnosis',\n",
    "    'primary_diagnosis',\n",
    "    'petersen_normal_cognitive_func',\n",
    "    'petersen_normal_daily_living',\n",
    "    'petersen_objective_ci',\n",
    "    'petersen_demented_by_diag',\n",
    "    'cause_mci',\n",
    "    'cause_dementia',\n",
    "    'cause_mci_non_ad_eth',\n",
    "    'cause_dementia_non_ad_eth',\n",
    "    'dementia_stage'\n",
    "]].copy()\n",
    "dx = dx.drop_duplicates().set_index(['subject_id', 'diag_date']).sort_index()\n",
    "\n",
    "# drop duplicated indices keeping the last\n",
    "dx = dx.groupby(['subject_id', 'diag_date']).nth(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95646cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add diagnostic changes information for 2Y and 4Y (using a +/- 6 month window)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    dx = calculateFutureVarValue(\n",
    "        df=dx,\n",
    "        y_followup=2,\n",
    "        m_window=6,\n",
    "        target_var='diagnosis',\n",
    "        group_id='subject_id',\n",
    "        date_id='diag_date'\n",
    "    )\n",
    "    dx = calculateFutureVarValue(\n",
    "        df=dx,\n",
    "        y_followup=4,\n",
    "        m_window=6,\n",
    "        target_var='diagnosis',\n",
    "        group_id='subject_id',\n",
    "        date_id='diag_date'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cf911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographic information\n",
    "demo_variables = {\n",
    "    'PTID': 'subject_id',\n",
    "    'PTEDUCAT': 'yschooling',\n",
    "    'PTGENDER': 'sex_1M_2F',\n",
    "    'PTDOB': 'date_of_birth',\n",
    "}\n",
    "demo = pd.read_csv(PATH_TO_DEMO)\n",
    "print('Initial shape: %d' % demo.shape[0])\n",
    "\n",
    "# process variables\n",
    "demo = demo.rename(columns=demo_variables)[list(demo_variables.values())]\n",
    "\n",
    "# drop entries with missing values\n",
    "demo = demo.dropna(how='any')\n",
    "demo = demo.loc[\n",
    "    (demo['yschooling'] > 0) &\n",
    "    demo['sex_1M_2F'].isin([1, 2])\n",
    "].copy()\n",
    "print('Shape after removing entries with missing information: %d' % demo.shape[0])\n",
    "\n",
    "# remove duplicated entries\n",
    "demo = demo.set_index('subject_id').groupby('subject_id').max()\n",
    "\n",
    "print('Shape after removing duplicates: %d' % demo.shape[0])\n",
    "\n",
    "# convert birth to datetime\n",
    "demo['date_of_birth'] = pd.to_datetime(demo['date_of_birth'], format='%m/%Y')\n",
    "\n",
    "# add demographic information to diagnostic data\n",
    "dx = dx.join(demo, how='left')\n",
    "\n",
    "# calculate subject age\n",
    "dx['age'] = (dx.index.get_level_values('diag_date') - dx['date_of_birth']).dt.days / 365.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(dx[['diagnosis', 'primary_diagnosis']].value_counts()).sort_index())\n",
    "display(pd.DataFrame(dx[['diagnosis', 'primary_diagnosis']].isna().sum(), columns=['n_missing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a66006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export diagnosis information\n",
    "dx.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_OUTPUT_DATA, '%s_diagnosis.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca5653",
   "metadata": {},
   "source": [
    "# Neuropsychological information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables of interest\n",
    "neurobat_variables = {\n",
    "    # Memory window\n",
    "    'AVDELTOT'       : 'memory_avlt_recognition',\n",
    "    'AVTOT1'         : 'memory_avlt_trial_1',\n",
    "    'AVTOT2'         : 'memory_avlt_trial_2',\n",
    "    'AVTOT3'         : 'memory_avlt_trial_3',\n",
    "    'AVTOT4'         : 'memory_avlt_trial_4',\n",
    "    'AVTOT5'         : 'memory_avlt_trial_5',\n",
    "    'AVTOT6'         : 'memory_avlt_trial_6',\n",
    "    'AVDEL30MIN'     : 'memory_avlt_delayed',\n",
    "\n",
    "    # Language window\n",
    "    'CATANIMSC'      : 'language_cat_fluency',\n",
    "    'CATANPERS'      : 'language_cat_perseveration',\n",
    "\n",
    "    'BNTSPONT': 'language_bnt_tot_correct_no_cue',\n",
    "    'BNTCSTIM': 'language_bnt_tot_correct_scue',\n",
    "    'BNTCPHON': 'language_bnt_tot_correct_pcue',\n",
    "    'BNTTOTAL': 'language_bnt_tot',\n",
    "\n",
    "    # Executive functions\n",
    "    'TRAASCOR'       : 'exec_tmt_a_time',\n",
    "    'TRABSCOR'       : 'exec_tmt_b_time',\n",
    "\n",
    "    # Visuospatial functioning\n",
    "    'CLOCKSCOR'      : 'visuos_clock_draw_tot_score',\n",
    "    'COPYSCOR'       : 'visuos_clock_copy_tot_score',\n",
    "\n",
    "    # attention\n",
    "    'DSPANFOR': 'attention_digit_span_forward',\n",
    "    'DSPANBAC': 'attention_digit_span_backward',\n",
    "}\n",
    "\n",
    "adas_1_variables = {\n",
    "    # Memory window\n",
    "    'Q1': 'memory_word_recall',\n",
    "    'Q4': 'memory_word_recall_delayed',\n",
    "    'Q8': 'memory_word_recognition',\n",
    "    'Q9': 'memory_remembering_test',\n",
    "\n",
    "    # Language window\n",
    "    'Q2': 'language_commands',\n",
    "    'Q5': 'language_naming',\n",
    "    'Q12': 'language_comprehension',\n",
    "    'Q11': 'language_word_finding_diff',\n",
    "    'Q10': 'language_spoken_language',\n",
    "\n",
    "    # Visuospatial window\n",
    "    'Q3': 'visuos_constructional_praxis',\n",
    "    'Q6': 'visuos_ideational_praxis',\n",
    "\n",
    "    # Attention window\n",
    "    'Q14': 'attention_number_cancellation'   # different coding\n",
    "}\n",
    "\n",
    "adas_go23_variables = {\n",
    "    # Memory window\n",
    "    'Q1SCORE': 'memory_word_recall',\n",
    "    'Q4SCORE': 'memory_word_recall_delayed',\n",
    "    'Q8SCORE': 'memory_word_recognition',\n",
    "    'Q9SCORE': 'memory_remembering_test',\n",
    "\n",
    "    # Language window\n",
    "    'Q2SCORE': 'language_commands',\n",
    "    'Q5SCORE': 'language_naming',\n",
    "    'Q10SCORE': 'language_comprehension',\n",
    "    'Q11SCORE': 'language_word_finding_diff',\n",
    "    'Q12SCORE': 'language_spoken_language',\n",
    "    \n",
    "    # Visuospatial window\n",
    "    'Q3SCORE': 'visuos_constructional_praxis',\n",
    "    'Q6SCORE': 'visuos_ideational_praxis',\n",
    "    \n",
    "    # Attention window\n",
    "    'Q13SCORE': 'attention_number_cancellation'   # different coding\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a404b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NEUROBAT information\n",
    "neurobat = pd.read_csv(PATH_TO_NEUROBAT)\n",
    "print('Initial shape: %d' % neurobat.shape[0])\n",
    "\n",
    "# fill EXAMDATE information\n",
    "neurobat.loc[neurobat['VISDATE'].isna(), 'VISDATE'] =\\\n",
    "    neurobat.loc[neurobat['VISDATE'].isna(), 'VISDATE']\n",
    "\n",
    "# remove entries without date\n",
    "neurobat = neurobat.dropna(subset=['VISDATE', 'PTID'])\n",
    "print('Shape after removing key variables: %d' % neurobat.shape[0])\n",
    "\n",
    "# process variables\n",
    "neurobat['VISDATE'] = pd.to_datetime(neurobat['VISDATE'])\n",
    "neurobat = neurobat.rename(columns={\n",
    "    'PTID': 'subject_id',\n",
    "    'VISDATE': 'neurobat_date'\n",
    "}).set_index(['subject_id', 'neurobat_date']).sort_index()\n",
    "\n",
    "# select variables\n",
    "neurobat = neurobat[list(neurobat_variables.keys())].rename(columns=neurobat_variables, errors='raise')\n",
    "neurobat = neurobat.dropna(how='all')\n",
    "\n",
    "# drop duplicates\n",
    "neurobat = neurobat.sort_index().groupby(['subject_id', 'neurobat_date']).nth(-1)\n",
    "\n",
    "print('Shape after removing missing in neuro vars: %d' % neurobat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5810df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ADAS information (ADNI 1)\n",
    "adas_1 = pd.read_csv(PATH_TO_ADAS_1)\n",
    "print('Initial shape (ADAS 1): %d' % adas_1.shape[0])\n",
    "\n",
    "# remove entries without date\n",
    "adas_1 = adas_1.dropna(subset=['EXAMDATE', 'PTID'])\n",
    "print('Shape after removing key variables (ADAS 1): %d' % adas_1.shape[0])\n",
    "\n",
    "# process variables\n",
    "adas_1['EXAMDATE'] = pd.to_datetime(adas_1['EXAMDATE'])\n",
    "adas_1 = adas_1.rename(columns={\n",
    "    'PTID': 'subject_id',\n",
    "    'EXAMDATE': 'adas_date'\n",
    "}).set_index(['subject_id', 'adas_date']).sort_index()\n",
    "\n",
    "adas_1 = adas_1[list(adas_1_variables.keys())].rename(columns=adas_1_variables, errors='raise')\n",
    "adas_1 = adas_1.dropna(how='all')\n",
    "\n",
    "print('Shape after removing missing in neuro vars: %d' % adas_1.shape[0])\n",
    "\n",
    "adas_go23 = pd.read_csv(PATH_TO_ADAS_GO23)\n",
    "print('Initial shape (ADAS GO,2,3): %d' % adas_go23.shape[0])\n",
    "\n",
    "# fill EXAMDATE information\n",
    "adas_go23.loc[adas_go23['DATE'].isna(), 'DATE'] =\\\n",
    "    adas_go23.loc[adas_go23['DATE'].isna(), 'VISDATE']\n",
    "\n",
    "# remove entries without date\n",
    "adas_go23 = adas_go23.dropna(subset=['DATE', 'PTID'])\n",
    "print('Shape after removing key variables (ADAS GO,2,3): %d' % adas_go23.shape[0])\n",
    "\n",
    "# process variables\n",
    "adas_go23['DATE'] = pd.to_datetime(adas_go23['DATE'])\n",
    "adas_go23 = adas_go23.rename(columns={\n",
    "    'PTID': 'subject_id',\n",
    "    'DATE': 'adas_date'\n",
    "}).set_index(['subject_id', 'adas_date']).sort_index()\n",
    "\n",
    "adas_go23 = adas_go23[list(adas_go23_variables.keys())].rename(columns=adas_go23_variables, errors='raise')\n",
    "adas_go23 = adas_go23.dropna(how='all')\n",
    "\n",
    "print('Shape after removing missing in neuro vars: %d' % adas_go23.shape[0])\n",
    "\n",
    "all_adas = pd.concat([adas_1, adas_go23], axis=0)\n",
    "\n",
    "assert all_adas.shape[1] == adas_1.shape[1]\n",
    "assert all_adas.shape[1] == adas_go23.shape[1]\n",
    "\n",
    "all_adas = all_adas.groupby(['subject_id', 'adas_date']).nth(-1)\n",
    "\n",
    "assert not all_adas.index.duplicated().any()\n",
    "\n",
    "print('Shape after merging all the information: %d' % all_adas.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ADAS y NEUROBAT (using a 30 day window)\n",
    "crossed_df = neurobat.join(all_adas, how='left').reset_index(['neurobat_date', 'adas_date'])\n",
    "crossed_df['_days_diff'] = (crossed_df['neurobat_date'] - crossed_df['adas_date']).dt.days.abs()\n",
    "crossed_df = crossed_df.reset_index().set_index(['subject_id', 'neurobat_date', '_days_diff']).sort_index()\n",
    "crossed_df = crossed_df.groupby(['subject_id', 'neurobat_date']).nth(0).reset_index('_days_diff')\n",
    "crossed_df.loc[crossed_df['_days_diff'] > 60, all_adas.columns] = np.nan\n",
    "crossed_df = crossed_df.drop(columns=['_days_diff'])\n",
    "\n",
    "assert not crossed_df.index.duplicated().any()\n",
    "\n",
    "print('Final dataframe shape: %d' % crossed_df.shape[0])\n",
    "final_neuro = crossed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264dc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the data\n",
    "final_neuro.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_OUTPUT_DATA, '%s_neuropsycho.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b45d62",
   "metadata": {},
   "source": [
    "# MMSE and CDR information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224292a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MMSE / CDR scores\n",
    "mmse = pd.read_csv(PATH_TO_MMSE)\n",
    "print('Initial shape: %d' % mmse.shape[0])\n",
    "\n",
    "# fill EXAMDATE information\n",
    "mmse.loc[mmse['VISDATE'].isna(), 'VISDATE'] =\\\n",
    "    mmse.loc[mmse['VISDATE'].isna(), 'USERDATE']\n",
    "\n",
    "# remove entries without date\n",
    "mmse = mmse.dropna(subset=['VISDATE', 'PTID'])\n",
    "print('Shape after removing key variables: %d' % mmse.shape[0])\n",
    "\n",
    "# process variables\n",
    "mmse['VISDATE'] = pd.to_datetime(mmse['VISDATE'])\n",
    "mmse = mmse.rename(columns={\n",
    "    'PTID': 'subject_id',\n",
    "    'VISDATE': 'mmse_date',\n",
    "    'MMSCORE': 'mmse'\n",
    "}).set_index(['subject_id', 'mmse_date']).sort_index()\n",
    "mmse = mmse[['mmse']].copy()\n",
    "mmse = mmse.dropna(how='all')\n",
    "\n",
    "# drop duplicates\n",
    "mmse = mmse.sort_index().groupby(['subject_id', 'mmse_date']).nth(-1)\n",
    "\n",
    "print('Shape after removing missing in neuro vars: %d' % mmse.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba162c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr = pd.read_csv(PATH_TO_CDR)\n",
    "print('Initial shape: %d' % cdr.shape[0])\n",
    "\n",
    "# remove entries without date\n",
    "cdr = cdr.dropna(subset=['VISDATE', 'PTID'])\n",
    "print('Shape after removing key variables: %d' % cdr.shape[0])\n",
    "\n",
    "# process variables\n",
    "cdr['VISDATE'] = pd.to_datetime(cdr['VISDATE'])\n",
    "cdr = cdr.rename(columns={\n",
    "    'PTID': 'subject_id',\n",
    "    'VISDATE': 'cdr_date',\n",
    "    'CDRSB': 'cdr'\n",
    "}).set_index(['subject_id', 'cdr_date']).sort_index()\n",
    "cdr = cdr[['cdr']].copy()\n",
    "cdr = cdr.dropna(how='all')\n",
    "\n",
    "# drop duplicates\n",
    "cdr = cdr.sort_index().groupby(['subject_id', 'cdr_date']).nth(-1)\n",
    "\n",
    "print('Shape after removing missing in neuro vars: %d' % cdr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a555b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ADAS y NEUROBAT (using a 30 day window)\n",
    "mmse_cdr_df = mmse.join(cdr).reset_index(['mmse_date', 'cdr_date'])\n",
    "mmse_cdr_df['_days_diff'] = (mmse_cdr_df['mmse_date'] - mmse_cdr_df['cdr_date']).dt.days.abs()\n",
    "mmse_cdr_df = mmse_cdr_df.reset_index().set_index(['subject_id', 'mmse_date', '_days_diff']).sort_index()\n",
    "mmse_cdr_df = mmse_cdr_df.groupby(['subject_id', 'mmse_date']).nth(0).reset_index('_days_diff')\n",
    "mmse_cdr_df.loc[mmse_cdr_df['_days_diff'] > 60, cdr.columns] = np.nan\n",
    "mmse_cdr_df = mmse_cdr_df.drop(columns=['_days_diff'])\n",
    "\n",
    "assert not mmse_cdr_df.index.duplicated().any()\n",
    "\n",
    "print('Final dataframe shape: %d' % mmse_cdr_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the data\n",
    "mmse_cdr_df.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_OUTPUT_DATA, '%s_mmse_cdr.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445030bc",
   "metadata": {},
   "source": [
    "# Neuroimaging metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about amyloid QC\n",
    "amy_qc = pd.read_csv(PATH_TO_AMY_QC1)\n",
    "amy_qc['SCANDATE'] = pd.to_datetime(amy_qc['SCANDATE'])\n",
    "amy_qc = amy_qc.set_index(['PTID', 'SCANDATE'])\n",
    "amy_qc = amy_qc[['SCANQLTY']]\n",
    "amy_qc.index.names = ['subject_id', 'amy_scan_date']\n",
    "amy_qc = amy_qc.rename(columns={'SCANQLTY': 'pass_amy_qc'})\n",
    "\n",
    "assert not amy_qc.isna().any().any()\n",
    "\n",
    "# information about AV45 QC\n",
    "av45_qc = pd.read_csv(PATH_TO_AMY_QC2)\n",
    "av45_qc['EXAMDATE'] = pd.to_datetime(av45_qc['EXAMDATE'])\n",
    "av45_qc = av45_qc.set_index(['PTID', 'EXAMDATE'])\n",
    "av45_qc = av45_qc[['PASS']]\n",
    "av45_qc.index.names = ['subject_id', 'amy_scan_date']\n",
    "av45_qc = av45_qc.rename(columns={'PASS': 'pass_amy_qc'})\n",
    "\n",
    "assert not av45_qc.isna().any().any()\n",
    "\n",
    "amy_qc = pd.concat([amy_qc, av45_qc], axis=0)\n",
    "\n",
    "amy_qc = amy_qc.groupby(['subject_id', 'amy_scan_date']).max()\n",
    "\n",
    "print(f'Number of entries in Amyloid QC: {amy_qc.shape[0]} (passed QC: {(amy_qc[\"pass_amy_qc\"].mean() * 100):.1f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the data\n",
    "amy_qc.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_OUTPUT_DATA, '%s_amy_qc.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a649bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about FDG QC\n",
    "fdg_qc = pd.read_csv(PATH_TO_FDG_QC1)\n",
    "fdg_qc['SCANDATE'] = pd.to_datetime(fdg_qc['SCANDATE'])\n",
    "fdg_qc = fdg_qc.set_index(['PTID', 'SCANDATE'])\n",
    "fdg_qc = fdg_qc[['SCANQLTY']]\n",
    "fdg_qc.index.names = ['subject_id', 'fdg_scan_date']\n",
    "fdg_qc = fdg_qc.rename(columns={'SCANQLTY': 'pass_fdg_qc'})\n",
    "\n",
    "assert not fdg_qc.isna().any().any()\n",
    "\n",
    "# information about fdg_a4 QC\n",
    "fdg_a4_qc = pd.read_csv(PATH_TO_FDG_QC2)\n",
    "fdg_a4_qc['EXAMDATE'] = pd.to_datetime(fdg_a4_qc['EXAMDATE'])\n",
    "fdg_a4_qc = fdg_a4_qc.set_index(['PTID', 'EXAMDATE'])\n",
    "fdg_a4_qc = fdg_a4_qc[['PASS']]\n",
    "fdg_a4_qc.index.names = ['subject_id', 'fdg_scan_date']\n",
    "fdg_a4_qc = fdg_a4_qc.rename(columns={'PASS': 'pass_fdg_qc'})\n",
    "\n",
    "assert not fdg_a4_qc.isna().any().any()\n",
    "\n",
    "fdg_qc = pd.concat([fdg_qc, fdg_a4_qc], axis=0)\n",
    "\n",
    "fdg_qc = fdg_qc.groupby(['subject_id', 'fdg_scan_date']).max()\n",
    "\n",
    "print(f'Number of entries in fdg QC: {fdg_qc.shape[0]} (passed QC: {(fdg_qc[\"pass_fdg_qc\"].mean() * 100):.1f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the data\n",
    "fdg_qc.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_OUTPUT_DATA, '%s_fdg_qc.parquet' % datetime.now().strftime('%Y%m%d'))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "mlv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
