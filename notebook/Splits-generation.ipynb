{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code used to generate the different splits used for training / validation and testing. Longitudinal data will be separated into training, validation and testing. On the other hand, cross-sectional data, as it is only for pre-training the models, will be divided into training and test **excluding all the subjects included in the longitudinal test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 9999\n",
    "PATH_TO_DATA = Path(os.path.join('..', 'data', 'mounts', 'v1'))\n",
    "PATH_TO_OUTPUT = Path(os.path.join('..', 'data', 'splits', 'v1'))\n",
    "PATH_TO_LONG = PATH_TO_DATA / '20240428_longitudinal.parquet'\n",
    "PATH_TO_CROSS_SECTIONAL = {\n",
    "    'mri': PATH_TO_DATA / '20240428_mri_cross_sectional.parquet',\n",
    "    'fdg': PATH_TO_DATA / '20240428_fdg_cross_sectional.parquet',\n",
    "    'amy': PATH_TO_DATA / '20240428_amy_cross_sectional.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "# load all the datasets\n",
    "long_df = pd.read_parquet(PATH_TO_LONG).sort_index()\n",
    "cross_dfs = {k: pd.read_parquet(f).sort_index() for k, f in PATH_TO_CROSS_SECTIONAL.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits for the longitudinal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we have the trajectories and there can be two trajectories for the same subject, \n",
    "# we must consider the splits at the subject level\n",
    "long_uids = long_df.index.get_level_values(0).unique()\n",
    "\n",
    "# stratify based on the number of cognitive domains affected considering this number as if they were different calses\n",
    "long_stratify = long_df.groupby('subject_id').nth(0)[[ \n",
    "    'lr_delta_memory_composite_binary',\n",
    "    'lr_delta_exec_composite_binary',\n",
    "    'lr_delta_language_composite_binary',\n",
    "    'lr_delta_visuospatial_composite_binary']].sum(axis=1)\n",
    "\n",
    "display(pd.DataFrame(long_stratify.value_counts()))\n",
    "\n",
    "# perform the split of the longitudinal dataset for train/test\n",
    "long_train, long_test, long_train_stratify, long_test_stratify = train_test_split(\n",
    "    long_uids, long_stratify, \n",
    "    test_size=0.3, random_state=RANDOM_SEED, shuffle=True, stratify=long_stratify\n",
    ")\n",
    "\n",
    "print('Data shape (train / test):\\n\\n\\ttrain: {}\\n\\ttest: {}\\n'.format(\n",
    "    long_train.shape, long_test.shape\n",
    "))\n",
    "print('\\n')\n",
    "\n",
    "# perform the split of the longitudinal dataset for train/validation\n",
    "long_train, long_valid, long_train_stratify, long_valid_stratify = train_test_split(\n",
    "    long_train, long_train_stratify, \n",
    "    test_size=0.15, random_state=RANDOM_SEED, shuffle=True, stratify=long_train_stratify\n",
    ")\n",
    "\n",
    "print('Data shape (train / valid):\\n\\n\\ttrain: {}\\n\\tvalid: {}\\n'.format(\n",
    "    long_train.shape, long_valid.shape\n",
    "))\n",
    "print('\\n')\n",
    "\n",
    "# select the final indices\n",
    "long_train_indices = long_df.loc[long_train].index\n",
    "long_valid_indices = long_df.loc[long_valid].index\n",
    "long_test_indices = long_df.loc[long_test].index\n",
    "\n",
    "print('Final number of entries: {} (train) | {} (valid) | {} (test)'.format(\n",
    "    long_train_indices.shape[0], long_valid_indices.shape[0], long_test_indices.shape[0]\n",
    "))\n",
    "\n",
    "# unitary tests\n",
    "assert len(set(long_test_indices.get_level_values(0).tolist()).intersection(\n",
    "    set(long_valid_indices.get_level_values(0).tolist())\n",
    ")) == 0\n",
    "\n",
    "assert len(set(long_test_indices.get_level_values(0).tolist()).intersection(\n",
    "    set(long_train_indices.get_level_values(0).tolist())\n",
    ")) == 0\n",
    "\n",
    "\n",
    "# create the final dataframe\n",
    "long_indices_df = pd.concat([\n",
    "    pd.DataFrame(['train']*len(long_train_indices), index=long_train_indices, columns=['split']),\n",
    "    pd.DataFrame(['valid']*len(long_valid_indices), index=long_valid_indices, columns=['split']),\n",
    "    pd.DataFrame(['test']*len(long_test_indices), index=long_test_indices, columns=['split']),\n",
    "], axis=0)\n",
    "\n",
    "pd.DataFrame(long_indices_df['split'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data distributions\n",
    "long_df_with_split = long_df.join(long_indices_df).copy()\n",
    "\n",
    "print('Average values')\n",
    "\n",
    "display(\n",
    "    long_df_with_split.groupby('split')[[\n",
    "        'lr_delta_memory_composite',\n",
    "        'lr_delta_exec_composite',\n",
    "        'lr_delta_language_composite',\n",
    "        'lr_delta_visuospatial_composite'\n",
    "    ]].mean().round(3)\n",
    ")\n",
    "\n",
    "print('SD values')\n",
    "display(\n",
    "    long_df_with_split.groupby('split')[[\n",
    "        'lr_delta_memory_composite',\n",
    "        'lr_delta_exec_composite',\n",
    "        'lr_delta_language_composite',\n",
    "        'lr_delta_visuospatial_composite'\n",
    "    ]].std().round(3)\n",
    ")\n",
    "\n",
    "print('Binary version')\n",
    "display(\n",
    "    (long_df_with_split.groupby('split')[[\n",
    "        'lr_delta_memory_composite_binary',\n",
    "        'lr_delta_exec_composite_binary',\n",
    "        'lr_delta_language_composite_binary',\n",
    "        'lr_delta_visuospatial_composite_binary'\n",
    "    ]].mean() * 100).astype(float).round(1)\n",
    ")\n",
    "\n",
    "print('Diagnosis information')\n",
    "display(\n",
    "    (pd.DataFrame(long_df_with_split.groupby('split')[[\n",
    "            'baseline_diagnosis',\n",
    "    ]].value_counts(normalize=True)) * 100).round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    (pd.DataFrame(long_df_with_split.groupby('split')[[\n",
    "            'diagnosis_2Y',\n",
    "    ]].value_counts(normalize=True)) * 100).round(2)\n",
    ")\n",
    "\n",
    "display(\n",
    "    (pd.DataFrame(long_df_with_split.groupby('split')[[\n",
    "            'diagnosis_4Y',\n",
    "    ]].value_counts(normalize=True)) * 100).round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the information\n",
    "in_long_date = re.findall('(^\\d{8}).*', os.path.split(PATH_TO_LONG)[1])[0]\n",
    "long_indices_df.to_parquet(\n",
    "    os.path.join(\n",
    "        PATH_TO_OUTPUT, '%s_longitudinal_generated%s.parquet' % (in_long_date, curr_date)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits for cross-sectional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_data_splits = {}\n",
    "for key, mod_df in cross_dfs.items():\n",
    "\n",
    "    # important!! prevent patients from the longitudinal test/validation dataset from being \n",
    "    # able to leak into the pre-training data\n",
    "    mod_df = mod_df.loc[\n",
    "        ~mod_df.index.get_level_values('subject_id').isin(\n",
    "            long_test_indices.get_level_values(0).unique().tolist()\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    # generate the splits\n",
    "    mod_df_train_index, mod_df_test_index = train_test_split(\n",
    "        mod_df.index,\n",
    "        test_size=0.15, random_state=RANDOM_SEED, shuffle=True\n",
    "    )\n",
    "    print('Split performed for \"{}\", entries: {} (train) | {} (valid)'.format(\n",
    "        key, mod_df_train_index.shape[0], mod_df_test_index.shape[0]))\n",
    "\n",
    "    # export the generated splits\n",
    "    in_cross_date = re.findall('(^\\d{8}).*', os.path.split(PATH_TO_CROSS_SECTIONAL[key])[1])[0]\n",
    "\n",
    "    cross_df_indices_df = pd.concat([\n",
    "        pd.DataFrame(['train']*len(mod_df_train_index), index=mod_df_train_index, columns=['split']),\n",
    "        pd.DataFrame(['test']*len(mod_df_test_index), index=mod_df_test_index, columns=['split'])\n",
    "    ], axis=0)\n",
    "    cross_df_indices_df.to_parquet(\n",
    "        os.path.join(\n",
    "            PATH_TO_OUTPUT, '%s_%s_cross_sectional_generated%s.parquet' % (in_cross_date, key, curr_date)\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
