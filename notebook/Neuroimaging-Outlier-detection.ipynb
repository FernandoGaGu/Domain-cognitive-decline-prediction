{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook used to detect artifacts in the processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where all the neuroimaging data is located\n",
    "PATH_TO_NEUROIMAGING_DATA = os.path.join('..', 'data', 'neuroimaging', 'v1')\n",
    "\n",
    "# path to the data generated using different pipelines, the key indicates pipeline priority \n",
    "# (lower = most priority)\n",
    "PATH_TO_MRI = {\n",
    "    0: os.path.join(PATH_TO_NEUROIMAGING_DATA, '20240428_MRI_intermodality_v0.parquet'),\n",
    "}\n",
    "PATH_TO_FDG = {\n",
    "    0: os.path.join(PATH_TO_NEUROIMAGING_DATA, '20240428_FDG_intermodality_v0.parquet'),\n",
    "}\n",
    "PATH_TO_AMY = {\n",
    "    0: os.path.join(PATH_TO_NEUROIMAGING_DATA, '20240428_AMY_intermodality_v0.parquet'),\n",
    "}\n",
    "\n",
    "# prefix that will be added to the generated parquets\n",
    "DATE_KEY = '20240428'\n",
    "\n",
    "# Parameter indicating whether to calculate statistics used to eliminate outliers \n",
    "# on statistics calculated on samples within the percentile delimited by this \n",
    "# parameter. (parameter selected ad-hoc for each modality)\n",
    "STATS_CUTOFF_VAL = 0.05\n",
    "\n",
    "# dpi used for image representation\n",
    "DPI = 100\n",
    "\n",
    "# number of jobs used to adjust the algorithm\n",
    "N_JOBS = 16\n",
    "\n",
    "# random seed\n",
    "SEED = 1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectOutliers(df: pd.DataFrame, title: str, cut_off: float = None):\n",
    "    \"\"\" Function used to detect outliers based on the Isolation Forest algorithm.\n",
    "    \n",
    "    This function will return the index of the subjects classified as outliers.\n",
    "    \"\"\"\n",
    "    iforest = IsolationForest(\n",
    "        n_estimators=600,\n",
    "        max_samples=0.75,\n",
    "        contamination='auto',\n",
    "        max_features=0.75,\n",
    "        bootstrap=False,\n",
    "        n_jobs=N_JOBS,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "\n",
    "    # fit isolation forest algorithm\n",
    "    iforest.fit(df.values)\n",
    "    \n",
    "    # get scores and calculate the cut-off considering 3 std from the trimmed mean\n",
    "    scores = iforest.score_samples(df.values)\n",
    "    \n",
    "    if cut_off is None:\n",
    "        cut_off = stats.trim_mean(scores, STATS_CUTOFF_VAL) - 3 * stats.mstats.trimmed_std(scores, (STATS_CUTOFF_VAL, STATS_CUTOFF_VAL))\n",
    "\n",
    "    # display statistics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    fig.set_dpi(DPI)\n",
    "    \n",
    "    # --- plot scores distribution\n",
    "    n_count, _, _ = axes[0].hist(scores, bins=50, color='#293462')\n",
    "    axes[0].axvline(cut_off, lw=2, color='#F24C4C', label='cut-off')\n",
    "    \n",
    "    axes[0].spines['top'].set_visible(False)\n",
    "    axes[0].spines['right'].set_visible(False)\n",
    "    \n",
    "    axes[0].fill_between(\n",
    "        [min(scores) * 1.1, cut_off],\n",
    "        [max(n_count) * 1.1, max(n_count) * 1.1],\n",
    "        alpha=0.3,\n",
    "        color='#F24C4C'\n",
    "    )\n",
    "    axes[0].set_ylim(0, max(n_count))\n",
    "    axes[0].set_xlim(min(scores)*1.1, max(scores)*0.9)\n",
    "    axes[0].set_xlabel('Score', size=13)\n",
    "    axes[0].set_ylabel('Number of samples', size=13)\n",
    "    axes[0].set_title('Outlier score (%s)' % title, size=15, pad=15)\n",
    "    \n",
    "    # --- plot PCA projection\n",
    "    zscore_data = (df - df.mean()) / df.std()\n",
    "    pca = PCA(\n",
    "        n_components=2,\n",
    "        random_state=SEED\n",
    "    ).fit(zscore_data)\n",
    "    pca_emb = pca.transform(zscore_data)\n",
    "    \n",
    "    axes[1].scatter(\n",
    "        pca_emb[scores >= cut_off, 0],\n",
    "        pca_emb[scores >= cut_off, 1],\n",
    "        s=20,\n",
    "        color='#293462'\n",
    "    )\n",
    "    axes[1].scatter(\n",
    "        pca_emb[scores < cut_off, 0],\n",
    "        pca_emb[scores < cut_off, 1],\n",
    "        s=20,\n",
    "        color='#F24C4C'\n",
    "    )\n",
    "    axes[1].set_xlabel('PC 1', size=12)\n",
    "    axes[1].set_ylabel('PC 2', size=12)\n",
    "    axes[1].set_title('PCA projection, EV %.3f (%s)' % (\n",
    "        np.cumsum(pca.explained_variance_ratio_)[-1], title), size=15, pad=15)\n",
    "    \n",
    "    for pos in ['right', 'top', 'left', 'bottom']:\n",
    "        axes[1].spines[pos].set_visible(False)\n",
    "    \n",
    "    # --- common layout\n",
    "    for ax in axes.flatten():\n",
    "        ax.grid(alpha=0.1, color='black')\n",
    "    \n",
    "    fig.suptitle(\n",
    "        'Number of outliers {}: {} ({:.2f}%)'.format(\n",
    "            title, \n",
    "            (scores < cut_off).sum(),\n",
    "            (scores < cut_off).sum() / len(scores) * 100\n",
    "        ), \n",
    "        size=17, y=1.15)\n",
    "    plt.show()\n",
    "\n",
    "    return df.loc[scores < cut_off].index\n",
    "\n",
    "\n",
    "def filterErrors(df: pd.DataFrame):\n",
    "    \"\"\" Filter out those examples that are 5 times above the interquartile range \n",
    "    in terms of the sum of all variables. These values can be safely classified as outliers. \"\"\"\n",
    "\n",
    "    # filter values with missing values\n",
    "    na_mask = df.isna().sum(axis=1) > 1\n",
    "    if na_mask.any():\n",
    "        print('Number of errors (missing values generated): %d' % na_mask.sum())\n",
    "    \n",
    "    median = df.median(axis=0)\n",
    "    q25 = df.quantile(0.25, axis=0)\n",
    "    q75 = df.quantile(0.75, axis=0)\n",
    "    iqr_range = 2.5 * (q75 - q25)\n",
    "\n",
    "    err_mask = ((df < (median - iqr_range)) | (df > (median + iqr_range))).sum(axis=1) > (df.shape[1] * 0.2)\n",
    "\n",
    "    print('Number of errors: %d' % err_mask.sum())\n",
    "\n",
    "    return df.loc[err_mask | na_mask].index\n",
    "\n",
    "\n",
    "def mergeDataFrames(source_df: pd.DataFrame, reference_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Merge outlier information from source and reference dataframe. \"\"\"\n",
    "\n",
    "    # avoid inplace modifications\n",
    "    source_df = source_df.copy()\n",
    "    reference_df = reference_df.copy()\n",
    "\n",
    "    # save input index names\n",
    "    index_names = source_df.index.names\n",
    "\n",
    "    # merge the information\n",
    "    source_df = source_df.reset_index(['date', 'acquisition_id'])\n",
    "    reference_df = reference_df.reset_index(['date', 'acquisition_id'])\n",
    "    merge_df = source_df.join(reference_df, how='inner', rsuffix='_reference')  \n",
    "\n",
    "    # select the entries closer in time\n",
    "    merge_df['days_diff'] = (merge_df['date'] - merge_df['date_reference']).dt.days.abs()\n",
    "    merge_df = merge_df\\\n",
    "        .reset_index().set_index(['subject_id', 'date', 'days_diff']).sort_index()\\\n",
    "        .groupby(['subject_id', 'date']).nth(0)\\\n",
    "        .reset_index().set_index(index_names)[['outlier', 'outlier_reference']]\n",
    "\n",
    "    assert merge_df.shape[0] == source_df.shape[0]\n",
    "\n",
    "    return merge_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MRI data\n",
    "mri = []\n",
    "for k, file in PATH_TO_MRI.items():\n",
    "    data_ = pd.read_parquet(file)\n",
    "    index_names = list(data_.index.names) \n",
    "    data_ = data_.reset_index()\n",
    "    data_['pipeline'] = k\n",
    "    data_ = data_.set_index(index_names + ['pipeline']).sort_index()\n",
    "    mri.append(data_)\n",
    "mri = pd.concat(mri, axis=0)\n",
    "\n",
    "print('Input data shape:', mri.shape)\n",
    "\n",
    "# filter errors based on simple statistics\n",
    "mri_filtered = filterErrors(mri)\n",
    "\n",
    "# apply outlier detection algorithm\n",
    "mri_outliers = detectOutliers(mri.loc[~mri.index.isin(mri_filtered)], 'MRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for discard FDG and Amyloid images based on MRI outlier information\n",
    "mri_filtered_df = pd.DataFrame(\n",
    "    np.zeros(shape=(len(mri_filtered))), \n",
    "    index=mri_filtered, \n",
    "    columns=['mri_placeholder'])\n",
    "mri_outliers_df = pd.DataFrame(\n",
    "    np.zeros(shape=(len(mri_outliers))), \n",
    "    index=mri_outliers, \n",
    "    columns=['mri_placeholder'])\n",
    "mri_outliers_df = pd.concat([mri_filtered_df, mri_outliers_df], axis=0)\n",
    "mri_outliers_df = mri_outliers_df.reset_index(['acquisition_id', 'pipeline']).drop(columns=['acquisition_id', 'pipeline'])\n",
    "mri_outliers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDG-PET outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FDG data\n",
    "fdg = []\n",
    "for k, file in PATH_TO_FDG.items():\n",
    "    data_ = pd.read_parquet(file)\n",
    "    index_names = list(data_.index.names) \n",
    "    data_ = data_.reset_index()\n",
    "    data_['pipeline'] = k\n",
    "    data_ = data_.set_index(index_names + ['pipeline']).sort_index()\n",
    "    fdg.append(data_)\n",
    "fdg = pd.concat(fdg, axis=0)\n",
    "\n",
    "# select mean and std values\n",
    "fdg = fdg[[c for c in fdg.columns if c.endswith('mean') or c.endswith('std')]].copy()\n",
    "print('Input data shape:', fdg.shape)\n",
    "\n",
    "# filter errors based on simple statistics\n",
    "fdg_filtered = filterErrors(fdg)\n",
    "\n",
    "# apply outlier detection algorithm\n",
    "fdg_outliers = detectOutliers(fdg.loc[~fdg.index.isin(fdg_filtered)], 'FDG-PET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AV45-PET outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AMY data\n",
    "amy = []\n",
    "for k, file in PATH_TO_AMY.items():\n",
    "    data_ = pd.read_parquet(file)\n",
    "    index_names = list(data_.index.names) \n",
    "    data_ = data_.reset_index()\n",
    "    data_['pipeline'] = k\n",
    "    data_ = data_.set_index(index_names + ['pipeline']).sort_index()\n",
    "    amy.append(data_)\n",
    "amy = pd.concat(amy, axis=0)\n",
    "\n",
    "# select mean and std values\n",
    "amy = amy[[c for c in amy.columns if c.endswith('mean') or c.endswith('std')]].copy()\n",
    "print('Input data shape:', amy.shape)\n",
    "\n",
    "# filter errors based on simple statistics\n",
    "amy_filtered = filterErrors(amy)\n",
    "\n",
    "# apply outlier detection algorithm\n",
    "amy_outliers = detectOutliers(amy.loc[~amy.index.isin(amy_filtered)], 'Amyloid-PET')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the generated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outlier info dataframes\n",
    "mri_outlier_info = pd.DataFrame(\n",
    "    np.zeros(shape=len(mri)).astype(int),\n",
    "    index=mri.index,\n",
    "    columns=['outlier']\n",
    ")\n",
    "\n",
    "fdg_outlier_info = pd.DataFrame(\n",
    "    np.zeros(shape=len(fdg)).astype(int),\n",
    "    index=fdg.index,\n",
    "    columns=['outlier']\n",
    ")\n",
    "\n",
    "amy_outlier_info = pd.DataFrame(\n",
    "    np.zeros(shape=len(amy)).astype(int),\n",
    "    index=amy.index,\n",
    "    columns=['outlier']\n",
    ")\n",
    "\n",
    "# mark outliers\n",
    "mri_outlier_info.loc[\n",
    "    mri_outlier_info.index.isin(mri_filtered) | \n",
    "    mri_outlier_info.index.isin(mri_outliers), 'outlier'\n",
    "] = 1\n",
    "fdg_outlier_info.loc[\n",
    "    fdg_outlier_info.index.isin(fdg_filtered) | \n",
    "    fdg_outlier_info.index.isin(fdg_outliers), 'outlier'\n",
    "] = 1\n",
    "amy_outlier_info.loc[\n",
    "    amy_outlier_info.index.isin(amy_filtered) | \n",
    "    amy_outlier_info.index.isin(amy_outliers), 'outlier'\n",
    "] = 1\n",
    "\n",
    "# display outlier information\n",
    "for name, df in [\n",
    "    ('mri', mri_outlier_info), \n",
    "    ('fdg', fdg_outlier_info), \n",
    "    ('amy', amy_outlier_info)]:\n",
    "\n",
    "    print('Outliers in {}: {} ({:.2f} %)'.format(\n",
    "        name,\n",
    "        df['outlier'].sum(),\n",
    "        df['outlier'].mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore pipeline information (only one pipeline has been used)\n",
    "mri_outlier_info = mri_outlier_info.reset_index('pipeline').drop(columns=['pipeline'])\n",
    "fdg_outlier_info = fdg_outlier_info.reset_index('pipeline').drop(columns=['pipeline'])\n",
    "amy_outlier_info = amy_outlier_info.reset_index('pipeline').drop(columns=['pipeline'])\n",
    "\n",
    "# add MRI outlier information to PET dataframes\n",
    "fdg_outlier_info = mergeDataFrames(fdg_outlier_info, mri_outlier_info)\n",
    "amy_outlier_info = mergeDataFrames(amy_outlier_info, mri_outlier_info)\n",
    "\n",
    "print('Percentage of outliers in MRI being also outliers in FDG-PET: {:.2f} %'.format(\n",
    "    float(\n",
    "        ((fdg_outlier_info['outlier'] == 1) & (fdg_outlier_info['outlier_reference'] == 1)).sum() / \n",
    "        (fdg_outlier_info['outlier']).sum() * 100\n",
    "    )\n",
    "))\n",
    "\n",
    "print('Percentage of outliers in MRI being also outliers in amyloid PET: {:.2f} %'.format(\n",
    "    float(\n",
    "        ((amy_outlier_info['outlier'] == 1) & (amy_outlier_info['outlier_reference'] == 1)).sum() / \n",
    "        (amy_outlier_info['outlier']).sum() * 100\n",
    "    )\n",
    "))\n",
    "\n",
    "# select PET images with marked as outliers in MRI as outliers\n",
    "fdg_outlier_info['outlier'] = (fdg_outlier_info.sum(axis=1) > 0).astype(int)\n",
    "amy_outlier_info['outlier'] = (amy_outlier_info.sum(axis=1) > 0).astype(int)\n",
    "\n",
    "# remove auxiliary columns\n",
    "fdg_outlier_info = fdg_outlier_info.drop(columns=['outlier_reference'])\n",
    "amy_outlier_info = amy_outlier_info.drop(columns=['outlier_reference'])\n",
    "\n",
    "# file with the indices associated with neuroimaging errors in the amyloid data\n",
    "# manually reviewed\n",
    "manual_amy_outliers = pd.read_csv(\n",
    "    os.path.join(\n",
    "        '..', '..', 'data', 'metadata', 'neuroimaging',\n",
    "        '20241106_reviewed_amyloid_errors.csv'\n",
    "    )\n",
    ")\n",
    "manual_amy_outliers['date'] = pd.to_datetime(manual_amy_outliers['date'])\n",
    "manual_amy_outliers = manual_amy_outliers.set_index(['subject_id', 'date', 'acquisition_id'])\n",
    "\n",
    "print('Number of errors manually reviewed for amyloid information')\n",
    "amy_outlier_info.loc[manual_amy_outliers.index, 'outlier'] = 1\n",
    "\n",
    "# display outlier information\n",
    "for name, df in [\n",
    "    ('mri', mri_outlier_info), \n",
    "    ('fdg', fdg_outlier_info), \n",
    "    ('amy', amy_outlier_info)]:\n",
    "\n",
    "    print('Outliers in {}: {} ({:.2f} %)'.format(\n",
    "        name,\n",
    "        df['outlier'].sum(),\n",
    "        df['outlier'].mean() * 100))\n",
    "    \n",
    "mri_outlier_info.shape, fdg_outlier_info.shape, amy_outlier_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the generated dataframes\n",
    "curr_date = datetime.now().strftime('%Y%m%d')\n",
    "amy_outlier_info.to_parquet(\n",
    "    os.path.join(PATH_TO_NEUROIMAGING_DATA, '%s_AMY_intermodality_v0_outliers_generated%s.parquet' % (DATE_KEY, curr_date)))\n",
    "mri_outlier_info.to_parquet(\n",
    "    os.path.join(PATH_TO_NEUROIMAGING_DATA, '%s_MRI_intermodality_v0_outliers_generated%s.parquet' % (DATE_KEY, curr_date)))\n",
    "fdg_outlier_info.to_parquet(\n",
    "    os.path.join(PATH_TO_NEUROIMAGING_DATA, '%s_FDG_intermodality_v0_outliers_generated%s.parquet' % (DATE_KEY, curr_date)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlv0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
